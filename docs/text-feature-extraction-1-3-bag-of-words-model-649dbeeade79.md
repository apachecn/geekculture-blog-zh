# æ–‡æœ¬ç‰¹å¾æå–(1/3):å•è¯è¢‹æ¨¡å‹

> åŸæ–‡ï¼š<https://medium.com/geekculture/text-feature-extraction-1-3-bag-of-words-model-649dbeeade79?source=collection_archive---------7----------------------->

![](img/61cab80d457efab17cda09216647c416.png)

[Source](https://unsplash.com/s/photos/cafe-study)

åœ¨ä»»ä½•æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ï¼Œç‰¹å¾éƒ½èµ·ç€ä¸»è¦ä½œç”¨ã€‚åªæœ‰å½“ç‰¹å¾æ˜¯æ•°å­—æ—¶ï¼Œè®­ç»ƒæ¨¡å‹æ‰æ˜¯å¯èƒ½çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰å„ç§æŠ€æœ¯å°†åˆ†ç±»ç¼–ç æˆæ•°å­—ï¼Œå¦‚æ ‡ç­¾ç¼–ç ã€ä¸€é”®ç¼–ç ã€æ•£åˆ—ç­‰ã€‚åŒæ ·ï¼Œåœ¨ NLP ä¸­ï¼ŒåŸºäºæ–‡æœ¬çš„æ¨¡å‹ä¹Ÿéœ€è¦ä½¿ç”¨å„ç§ç‰¹å¾æå–æŠ€æœ¯(å¦‚å•è¯è¢‹(BOW)ã€TF-IDF æˆ–å•è¯åµŒå…¥)æ¥è·å¾—æ•°å­—ç‰¹å¾ã€‚

è¿™ä¸ªåšå®¢ç³»åˆ—å°†è§£é‡Šå¦‚ä½•ä½¿ç”¨ BOWã€TFIDF å’Œå•è¯åµŒå…¥è·å¾—åŸºäºæ–‡æœ¬çš„æ•°å­—ç‰¹å¾ã€‚æˆ‘ä»¬å°†åœ¨è¿™ä¸ªåšå®¢ä¸­è®¨è®ºå¼“æ¨¡å‹ã€‚BOW æŠ€æœ¯æ˜¯æœ€åŸºæœ¬çš„ç‰¹å¾æå–å’Œæœ€æ—©çš„æ–¹æ³•ã€‚æœ€å®¹æ˜“ç†è§£å’Œå®ç°ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¼€å§‹å§ã€‚

## ç›´è§‰

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæ–‡æ¡£çš„é›†åˆç§°ä¸ºè¯­æ–™åº“ï¼Œè€Œæ ‡è®°(è¯)çš„é›†åˆç§°ä¸ºæ–‡æ¡£ã€‚è¿™æ„å‘³ç€å…·æœ‰ç›¸ä¼¼å•è¯çš„æ–‡æ¡£åº”è¯¥æ˜¯ç›¸ä¼¼çš„ã€‚

## ç†è®ºä¸æ¦‚å¿µ

*   å•è¯åŒ…æ˜¯ä¸€ç§å°†**æ–‡æœ¬æ•°æ®è½¬æ¢æˆæ•°å­—å‘é‡**ä½œä¸ºç‰¹å¾çš„ç‰¹å¾æå–æ–¹æ³•
*   è¿™äº›æ•°å­—æ˜¯æ–‡æ¡£ä¸­æ¯ä¸ªå•è¯(æ ‡è®°)çš„è®¡æ•°
*   ç”Ÿæˆç±»å‹ä¸º *scipy.sparse.csr.matrix* çš„**ç¨€ç–çŸ©é˜µ**(å¤§éƒ¨åˆ†ä¸º 0)
*   [ä¸‹é¢çš„ CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) æä¾›äº†æŸäº›å‚æ•°ï¼Œè¿™äº›å‚æ•°èƒ½å¤Ÿæ‰§è¡Œæ•°æ®é¢„å¤„ç†ï¼Œå¦‚åœæ­¢å­—ã€ä»¤ç‰Œæ¨¡å¼ã€lower ç­‰ã€‚å®ƒåœ¨ä»¤ç‰Œå¤§å°ä¸Šæ˜¯çµæ´»çš„ï¼Œå› ä¸ºé»˜è®¤çš„ ngram_range è¡¨ç¤º 1 ä¸ªå•è¯ï¼Œä½†å®ƒå¯ä»¥æ ¹æ®ä½¿ç”¨æƒ…å†µè¿›è¡Œæ›´æ”¹ã€‚

> *class* `sklearn.feature_extraction.text.**CountVectorizer**` ( *** ï¼Œ *input='content'* ï¼Œ *encoding='utf-8'* ï¼Œ *decode_error='strict'* ï¼Œ *strip_accents=None* ï¼Œ *lowercase=True* ï¼Œ*é¢„å¤„ç†ç¨‹åº=None* ï¼Œ *tokenizer=None* ï¼Œ *stop_wordsu)\b\w\w+\b'* ï¼Œ *ngram_range=(1* ï¼Œ *1)* ï¼Œ *analyzer='word'* ï¼Œ *max_df=1.0* ï¼Œ *min_df=1* ï¼Œ *max_features=None* ï¼Œ *vocabulary=None* ï¼Œ *binary=False* ï¼Œ

## *è¿‡ç¨‹*

*è®©æˆ‘ä»¬åˆ‡å…¥æ­£é¢˜ï¼Œç ”ç©¶ä¸€ä¸‹å»ºç«‹å•è¯è¢‹æ¨¡å‹çš„æ­¥éª¤ã€‚
***æ­£æ–‡:*** æˆ‘æ˜¯æ²™æ± ï¼Œæˆ‘æ˜¯å­¦ä¹ è€…*

1.  *æ–‡æœ¬è¢«**æ ‡è®°åŒ–**
    â€œæˆ‘â€ã€â€œæˆ‘â€ã€â€œæˆ‘â€ã€â€œæ²™æ± â€ã€â€œå’Œâ€ã€â€œæˆ‘â€ã€â€œæˆ‘â€ã€â€œæˆ‘â€ã€â€œleanerâ€*
2.  *æ‰¾å‡ºç‹¬ç‰¹çš„å•è¯ï¼Œæ„å»ºä¸€ä¸ª**è¯æ±‡**
    ã€æˆ‘ã€‘ã€amã€‘ã€æ²™æ± ã€‘ã€å’Œã€‘ã€å­¦ä¹ è€…ã€‘*
3.  *é€šè¿‡ä¸€é”®ç¼–ç å°†è®°å·è½¬æ¢æˆæ•°å­—å‘é‡ï¼Œå¹¶**è®¡æ•°æ¯ä¸ªè¯æ±‡å•è¯** {â€œIâ€:2ï¼Œâ€œamâ€:2ï¼Œâ€œæ²™æ± â€:1ï¼Œâ€œandâ€:1ï¼Œâ€œå­¦ä¹ è€…â€:1}*

## *å±¥è¡Œ*

*ä¸ºäº†ç†è§£ BOW æ¨¡å‹ï¼Œè®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å¦‚ä½•æ‰‹åŠ¨å®ç°ï¼Œç„¶å will for Sklearn å®ç°ã€‚*

1.  ***æ‰‹åŠ¨***

*   *è®©æˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„å¥å­è¯­æ–™åº“ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢æˆå°å†™ï¼Œä»¥ä¾¿ä¸åŒºåˆ†â€œthisâ€å’Œâ€œThisâ€ã€‚*

```
*doc = ["This is a good city",
      "You are good human",
      "This is worth fight for your own worth"]
#Convert into lowercase
doc = list(map(str.lower, doc))*
```

*   *æ ‡è®°å¹¶åˆ›å»ºä¸€ä¸ªè¯†åˆ«ç‹¬ç‰¹å•è¯çš„è¯æ±‡è¡¨ã€‚
    ä¸ºè¯æ±‡è®¾ç½®ç´¢å¼•*

```
*unique_words = set((doc[0] + ' '+ doc[1] +' '+ doc[2]).split())
index_dict = {}
for ind, i in enumerate(sorted(unique_words)):
    index_dict[i] = ind
"""
{'a': 0,
 'are': 1,
 'city': 2,
 'fight': 3,
 'for': 4,
 'good': 5,
 'human': 6,
 'is': 7,
 'own': 8,
 'this': 9,
 'worth': 10,
 'you': 11,
 'your': 12}
"""*
```

*   *åˆ›å»ºä¸€ä¸ªç¨€ç–çŸ©é˜µ
    â€”â€”éå†è¯­æ–™åº“ä¸­çš„æ¯ä¸ªæ–‡æ¡£(å¥å­)ã€‚è·å–å®ƒçš„å­—æ•°ã€‚
    -åˆ›å»ºä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œä¸ºè¡Œã€åˆ—å’Œå€¼åˆ›å»ºå˜é‡ã€‚
    â€”è·å–ä¸è¯­æ–™åº“ç›¸å…³çš„â€œè¡Œâ€æ•°æ®
    â€”è·å–â€œåˆ—â€æ•°æ®ä½œä¸ºä¸ index_dict ç›¸å…³çš„ç´¢å¼•
    â€”è·å–â€œå€¼â€æ•°æ®ä½œä¸ºä¸ count_dict ç›¸å…³çš„è®¡æ•°*

```
*from scipy.sparse import csr_matrixrow,col,val = [],[],[]
for idx, text in enumerate(doc):
    count_dict = {}
    tokens = text.split()
    # Get count of each word in sentence
    for word in tokens:        
        count_dict[word] = tokens.count(word)   

    for word, count in count_dict.items():
        ind = index_dict[word]        
        row.append(idx)
        col.append(ind)
        val.append(count)
print((csr_matrix((val, (row, col)),shape = (len(doc),len(index_dict)))).toarray())"""
[[1 0 1 0 0 1 0 1 0 1 0 0 0]
 [0 1 0 0 0 1 1 0 0 0 0 1 0]
 [0 0 0 1 1 0 0 1 1 1 2 0 1]]
"""*
```

***2ã€‚ä¸Šé¢çš„æ­¥éª¤åªç”¨ä¸¤è¡Œä»£ç å°±å¯ä»¥å®Œæˆã€‚Scikit-learn æä¾›äº†ä¸€ä¸ªåä¸º Count Vectorizer çš„åº“ï¼Œéœ€è¦åœ¨è¯­æ–™åº“ä¸Šè¿›è¡Œæ‹Ÿåˆå’Œè½¬æ¢ã€‚***

```
*cv = CountVectorizer(token_pattern=r"(?u)\b\w+\b")
count_occurrences = cv.fit_transform(doc)
count_occurrences.toarray()
"""
array([[1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],
       [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0],
       [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1]], dtype=int64)
"""*
```

## *ç¼ºç‚¹*

*   ***è¯­ä¹‰**:å› ä¸ºåªè€ƒè™‘å­—æ•°ï¼Œæ‰€ä»¥æœ‰è¯­ä¹‰ã€ç»“æ„æˆ–è¯­æ³•æ„ä¹‰*
*   ***ç¨€ç–çŸ©é˜µ** : BOW äº§ç”Ÿä¸€ä¸ªç¨€ç–çŸ©é˜µ(å¤§éƒ¨åˆ†æ˜¯ 0)ã€‚ä»»ä½•ç¨€ç–æ¨¡å‹éƒ½å¾ˆéš¾å»ºæ¨¡ï¼Œå› ä¸ºå®ƒéœ€è¦å¤§é‡çš„å†…å­˜å’Œè®¡ç®—èµ„æºã€‚*

*ä¸ºäº†å…‹æœä¸Šè¿°ç¼ºç‚¹ï¼Œå¼•å…¥äº† TF-IDF ç‰¹å¾æå–æ–¹æ³•ï¼Œè¿™æ˜¯æ–‡æœ¬ç‰¹å¾æå–ç³»åˆ—çš„ä¸‹ä¸€éƒ¨åˆ†ã€‚*

***çœ‹çœ‹æˆ‘çš„**[**GitHub repo**](https://github.com/shachi01/NLP/blob/main/BagOfWords_Scratch_Sklearn.ipynb)**æ€»ç»“äº†è¿™é‡Œæ¼”ç¤ºçš„æ‰€æœ‰ä»£ç ã€‚***

***æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡æœ¬** [**GitHub èµ„æºåº“**](https://github.com/shachi01/NLP/blob/main/BagOfWords_MovieReviews.ipynb) **äº†è§£æ›´å¤šå…³äºå¦‚ä½•å¼€å‘ä¸€ä¸ªè¯è¢‹æ¨¡å‹æ¥é¢„æµ‹ç”µå½±è¯„è®ºæƒ…ç»ªçš„ä¿¡æ¯ã€‚***

*å¦‚æœä½ å–œæ¬¢è¿™ä½ä½œè€…çš„åšå®¢ï¼Œè¯·éšæ„å…³æ³¨ï¼Œå› ä¸ºè¿™ä½ä½œè€…å‘ä½ ä¿è¯ä¼šå¸¦æ¥æ›´å¤šæœ‰è¶£çš„äººå·¥æ™ºèƒ½ç›¸å…³çš„ä¸œè¥¿ã€‚
è°¢è°¢ï¼Œ
å­¦ä¹ æ„‰å¿«ï¼ğŸ˜„*

****å¯ä»¥é€šè¿‡***[***LinkedIn***](https://www.linkedin.com/in/kaul-shachi)***å–å¾—è”ç³»ã€‚****