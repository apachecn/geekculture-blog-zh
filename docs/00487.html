<html>
<head>
<title>Hugging Face DistilBert &amp; Tensorflow for Custom Text Classification.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ç”¨äºè‡ªå®šä¹‰æ–‡æœ¬åˆ†ç±»çš„æ‹¥æŠ±è„¸è’¸é¦å’Œå¼ é‡æµã€‚</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7?source=collection_archive---------0-----------------------#2021-02-18">https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7?source=collection_archive---------0-----------------------#2021-02-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="3811" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">é€æ­¥æŒ‡å—</h2><div class=""/><div class=""><h2 id="6f6f" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">å¦‚ä½•é€šè¿‡TensorFlowçš„æ‹¥æŠ±è„¸APIå¾®è°ƒæ–‡æœ¬äºŒè¿›åˆ¶åˆ†ç±»çš„DistilBERTï¼Ÿ</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/78526cf363bb1a9d856e205070aa96c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drciYNtmeyyeF4WNNXMlcg.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@ninjason?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jason Leung</a> on <a class="ae jw" href="https://unsplash.com/s/photos/bookshelf?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="c60b" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">ä»‹ç»ã€‚</h1><p id="343f" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°ä¸€ä¸ªä½¿ç”¨<a class="ae jw" href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/#:~:text=Transfer%20learning%20is%20a%20machine,model%20on%20a%20second%20task." rel="noopener ugc nofollow" target="_blank">è¿ç§»å­¦ä¹ </a>æŠ€æœ¯çš„äºŒè¿›åˆ¶æ–‡æœ¬åˆ†ç±»å®ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<em class="ll"> DistilBertï¼Œä¸€ä¸ªæ¥è‡ªæ‹¥æŠ±è„¸<em class="ll">å˜å½¢é‡‘åˆš</em>åº“çš„</em>é¢„è®­ç»ƒæ¨¡å‹åŠå…¶ç”¨äº<em class="ll"> Tensorflow </em>çš„APIã€‚</p><h1 id="1abd" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">ä¸ºä»€ä¹ˆè’¸é¦ã€‚</h1><blockquote class="lm ln lo"><p id="d518" class="kp kq ll kr b ks lp is ku kv lq iv kx lr ls la lb lt lu le lf lv lw li lj lk hb bi translated"><a class="ae jw" rel="noopener" href="/huggingface/distilbert-8cf3380435b5">æ›´å°ã€æ›´å¿«ã€æ›´ä¾¿å®œã€æ›´è½»:æ¨å‡ºè’¸é¦ç‰ˆDistilBERTã€‚</a></p></blockquote><p id="8ca9" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">Mediumä¸Šçš„<em class="ll">æ‹¥æŠ±è„¸</em>çš„è¯„è®ºæ–‡ç« æ ‡é¢˜ç»™å‡ºäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥åœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸­ä½¿ç”¨è¿™ä¸ªæ¨¡å‹çš„å®Œæ•´è§£é‡Šã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå°çš„æ•°æ®é›†ï¼Œè¿™ä¸ªæ¨¡å‹å¯èƒ½æ˜¯æˆ‘ä»¬å°è¯•çš„ä¸€ä¸ªä¸é”™çš„é¦–é€‰ã€‚å¦å¤–ï¼Œ<a class="ae jw" href="https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8" rel="noopener" target="_blank">å¦ä¸€ç¯‡å…³äºMedium </a>çš„æ–‡ç« å»ºè®®ä½¿ç”¨<em class="ll"> DistilBERT </em>ä½œä¸ºå¿«é€ŸåŸºçº¿æ¨¡å‹ã€‚<em class="ll"> DistilBERT </em>å¯ä»¥åœ¨<em class="ll"> BERT </em>çš„è¡¨ç°ä¸Šè¾¾åˆ°ä¸€ä¸ªåˆç†çš„ä¸‹é™ï¼Œå…·æœ‰è®­ç»ƒæ›´å¿«çš„ä¼˜åŠ¿ã€‚è¿™ä¸ªAPIæˆ‘ä»¬åœ¨ä¸€ä¸ªå†™å¾—éå¸¸å¥½çš„<a class="ae jw" href="https://huggingface.co/transformers/master/model_doc/distilbert.html" rel="noopener ugc nofollow" target="_blank"> <em class="ll">æŠ±æŠ±è„¸</em>æ–‡æ¡£</a>å’Œ<a class="ae jw" href="https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html" rel="noopener ugc nofollow" target="_blank"> <em class="ll"> Tensorflow </em>åšå®¢</a>ã€‚ä½ ä¼šå‘ç°åº”ç”¨å®ƒæ˜¯å¤šä¹ˆç®€å•å’Œç›´è§‚ã€‚</p><blockquote class="lm ln lo"><p id="024f" class="kp kq ll kr b ks lp is ku kv lq iv kx lr ls la lb lt lu le lf lv lw li lj lk hb bi translated">ä¸€ä¸ªæ›´å°çš„é€šç”¨è¯­è¨€è¡¨ç¤ºæ¨¡å‹ï¼Œç§°ä¸ºDistilBERTï¼Œå®ƒå¯ä»¥åƒå®ƒçš„æ›´å¤§å¯¹åº”ç‰©ä¸€æ ·åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸Šæœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚</p></blockquote><p id="c640" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">å¦‚æœä½ è¿˜åœ¨çŠ¹è±«ä»<em class="ll">æŠ±è„¸</em>åº“ä¸­é€‰æ‹©å“ªä¸ªæ¨¡å‹ï¼Œä½ å¯ä»¥ç”¨ä»–ä»¬çš„<a class="ae jw" href="https://huggingface.co/models?filter=en,tf&amp;pipeline_tag=text-classification" rel="noopener ugc nofollow" target="_blank">è¿‡æ»¤å™¨</a>æŒ‰ä»»åŠ¡ã€åº“ã€è¯­è¨€ç­‰é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ã€‚<em class="ll"> DistilBERT </em>æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡<em class="ll">åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ª(</em>æ˜¯<a class="ae jw" href="https://huggingface.co/distilbert-base-uncased" rel="noopener ugc nofollow" target="_blank">distil Bert-base-uncased</a>çš„ä¸€ä¸ªå¾®è°ƒæ£€æŸ¥ç‚¹ï¼Œåœ¨SST-2ä¸Šå¾®è°ƒ)ã€‚æ‰€ä»¥æˆ‘ä»¬é€‰æ‹©äº†å®ƒâ€”â€”å¤ªå¥½äº†ï¼</p><h1 id="34e9" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">å¯¹æ•°æ®çš„å›é¡¾ã€‚</h1><p id="75b1" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">ä»£ç ç¤ºä¾‹çš„æ•°æ®æ¥è‡ªæˆ‘ä¹‹å‰çš„é¡¹ç›®ã€‚æˆ‘ä»ä¸€ä¸ªèœè°±ç½‘ç«™ä¸Šæ”¶é›†çš„ï¼Œæ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†(æ‹†åˆ†æ¯”ä¾‹0.2)ã€‚æ•°æ®é›†åŒ…å«ä¸€åˆ—æºæ–‡æœ¬(ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ•°æ®é›†<a class="ae jw" rel="noopener" href="/codestory/nlp-text-pre-processing-and-feature-engineering-python-69338fa0372e">æˆ–æŸ¥çœ‹è¿™ä¸ª</a><a class="ae jw" href="https://colab.research.google.com/drive/1YOZ60sdOjSbIiB3IiJVUYprQhJDEVxXB?usp=sharing" rel="noopener ugc nofollow" target="_blank">ç¬”è®°æœ¬</a>)ã€‚è¿˜æœ‰ä¸€ä¸ªå¸¦æ ‡ç­¾çš„åˆ—<em class="ll">ã€‚</em><strong class="kr hs">ä¸šåŠ¡ç›®æ ‡</strong>æ˜¯ç¡®å®šæ¯æ®µçš„æ ‡ç­¾æ˜¯<em class="ll">â€œé…æ–™â€</em>è¿˜æ˜¯<em class="ll">â€œé…æ–¹è¯´æ˜â€ã€‚</em></p><p id="7086" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">è®©æˆ‘ä»¬å®‰è£…ã€å¯¼å…¥åº“ï¼Œå¹¶ä¸ºæ¨¡å‹çš„è¶…å‚æ•°å®šä¹‰å¸¸æ•°:</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="ea73" class="mc jy hi ly b fi md me l mf mg">!pip install transformers</span><span id="14ac" class="mc jy hi ly b fi mh me l mf mg">import pandas as pd<br/>import tensorflow as tf<br/>import transformers<br/>from transformers import DistilBertTokenizer<br/>from transformers import TFDistilBertForSequenceClassification</span><span id="18e9" class="mc jy hi ly b fi mh me l mf mg">pd.set_option('display.max_colwidth', None)<br/>MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'<br/>BATCH_SIZE = 16<br/>N_EPOCHS = 3</span></pre><p id="cc70" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">ç›®å‰ï¼Œæˆ‘ä»¬åªå¯¹â€œæ®µè½â€å’Œâ€œæ ‡ç­¾â€åˆ—æ„Ÿå…´è¶£ã€‚çœ‹ä¸‹å›¾(å›¾1):â€œæ®µè½â€ä¸­çš„æ–‡æœ¬æ˜¯ä¸€ä¸ªæºæ–‡æœ¬ï¼Œè€Œä¸”æ˜¯ç”¨å­—èŠ‚è¡¨ç¤ºçš„ã€‚åœ¨X_trainé›†åˆä¸­ï¼Œæˆ‘ä»¬æœ‰3898è¡Œï¼ŒX_testé›†åˆæœ‰973è¡Œã€‚åœ¨è¿™äº›é›†åˆä¸­æ²¡æœ‰NaNsæˆ–ç©ºå­—ç¬¦ä¸²ã€‚</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mi"><img src="../Images/f089ab863523e25be6321871817014b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1hON-1TM81e1EWVDPK4n1Q.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic.1 Load Train and Test data sets, a sample from X_train, shape check.</figcaption></figure><p id="8892" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">å¦‚æœæ®µè½æ˜¯â€œé…æ–¹æˆåˆ†â€ï¼Œç›®æ ‡å˜é‡æ˜¯â€œ1â€ï¼Œå¦‚æœæ˜¯â€œè¯´æ˜â€ï¼Œç›®æ ‡å˜é‡æ˜¯â€œ0â€ã€‚æ ‡ç­¾çš„æ¯”ä¾‹å¤§çº¦æ˜¯20% 1å’Œ80% 0ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿›å…¥ä¸‹ä¸€æ­¥ã€‚</p><h1 id="c95c" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">å‡†å¤‡æ•°æ®ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚</h1><p id="cda2" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">åœ¨æ–‡æœ¬æˆä¸ºæ¨¡å‹è¾“å…¥ä¹‹å‰ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥å¯¹å®ƒè¿›è¡Œæ ‡è®°åŒ–ã€‚<a class="ae jw" href="https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer" rel="noopener ugc nofollow" target="_blank"><em class="ll">distilbertokenizer</em></a>æ¥å—â€œstrâ€(å•ä¸ªç¤ºä¾‹)ã€â€œList[str]â€(æ‰¹é‡æˆ–å•ä¸ªé¢„æ ‡è®°ç¤ºä¾‹)æˆ–â€œList[List[str]]â€(æ‰¹é‡é¢„æ ‡è®°ç¤ºä¾‹)ç±»å‹çš„æ–‡æœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†å­—èŠ‚è¡¨ç¤ºè½¬æ¢æˆå­—ç¬¦ä¸²ã€‚Lambdaå‡½æ•°æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è§£å†³æ–¹æ¡ˆã€‚</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="af9f" class="mc jy hi ly b fi md me l mf mg">X_train = X_train.apply(lambda x: str(x[0], 'utf-8'))<br/>X_test = X_test.apply(lambda x:  str(x[0], 'utf-8'))</span></pre><p id="826d" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">DistilBERTä¸­æœ€å¤§æ”¯æŒçš„æ ‡è®°åŒ–å¥å­é•¿åº¦æ˜¯512ä¸ªå•è¯ã€‚</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="33e3" class="mc jy hi ly b fi md me l mf mg"><em class="ll">#define a tokenizer object<br/></em>tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)</span><span id="36ee" class="mc jy hi ly b fi mh me l mf mg">#tokenize the text<br/>train_encodings = tokenizer(list(X_train.values),<br/>                            truncation=True, <br/>                            padding=True)</span><span id="2dfd" class="mc jy hi ly b fi mh me l mf mg">test_encodings = tokenizer(list(X_test.values),<br/>                           truncation=True, <br/>                           padding=True)</span></pre><p id="ab1d" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">æˆ‘ä»¬ä¼ é€’ç»™è®°å·èµ‹äºˆå™¨çš„å‚æ•°hold:æˆ‘ä»¬çš„set in "list[str]"è¡¨ç¤ºï¼Œtruncation=Trueï¼Œpadding=Trueã€‚å¦‚æœæ ‡è®°åŒ–å¥å­é•¿åº¦å°äºæœ€å¤§æ¨¡å‹è¾“å…¥é•¿åº¦ï¼Œæ ‡è®°åŒ–å™¨å°†å…¶æˆªæ–­åˆ°æ ‡è®°åŒ–å¥å­æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ ‡è®°åŒ–çš„å¥å­é•¿åº¦å°äºæœ€å¤§æ ‡è®°åŒ–çš„å¥å­é•¿åº¦ï¼Œæ ‡è®°åŒ–å™¨ç”¨é›¶å¡«å……ï¼Œç›´åˆ°æœ€å¤§æ ‡è®°åŒ–çš„å¥å­é•¿åº¦ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ç»“æœç¤ºä¾‹:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mj"><img src="../Images/a1e04486ccd83c615591d495e5bb2de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sU-GnyDLPcpEOBiyyHGY1g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic2. An example of tokenized sentece by DistilBertTokenizer.</figcaption></figure><p id="4c5f" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">DistilBertTokenizerå¼•ç”¨è¶…ç±»<code class="du mk ml mm ly b"><a class="ae jw" href="https://huggingface.co/transformers/model_doc/bert.html#transformers.BertTokenizer" rel="noopener ugc nofollow" target="_blank"><strong class="kr hs">BertTokenizer</strong></a><strong class="kr hs">.</strong></code>ï¼Œå®ƒè¿”å›ç»™æˆ‘ä»¬ä¸€ç»„è¾“å…¥<a class="ae jw" href="https://huggingface.co/transformers/glossary.html#input-ids" rel="noopener ugc nofollow" target="_blank">ç´¢å¼•</a>å’Œ<a class="ae jw" href="https://huggingface.co/transformers/glossary.html#attention-mask" rel="noopener ugc nofollow" target="_blank">æ³¨æ„å±è”½</a>ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬åªéœ€è¦å°†æ ‡ç­¾å’Œç¼–ç è½¬æ¢æˆTensorflowæ•°æ®é›†å¯¹è±¡:</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="1846" class="mc jy hi ly b fi md me l mf mg">train_dataset = <br/>tf.data.Dataset.from_tensor_slices((dict(train_encodings),<br/>                                    list(y_train.values)))<br/>test_dataset = <br/>tf.data.Dataset.from_tensor_slices((dict(test_encodings),<br/>                                    list(y_test.values)))</span></pre><h1 id="7c3a" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">ä½¿ç”¨native TensorFlowè¿›è¡Œå¾®è°ƒã€‚</h1><p id="f607" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨TFDistilBertForSequenceClassificationï¼Œå¹¶å°†æ¨¡å‹çš„åç§°ä½œä¸ºå‚æ•°ã€‚è®¾ç½®å­¦ä¹ ç‡å¹¶å®šä¹‰æŸå¤±å‡½æ•°ã€‚ç¼–è¯‘æ¨¡å‹å¹¶è¿è¡Œmodel.fit()æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="820f" class="mc jy hi ly b fi md me l mf mg">model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)</span><span id="4c36" class="mc jy hi ly b fi mh me l mf mg"><em class="ll">#chose the optimizer<br/></em>optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)</span><span id="5b65" class="mc jy hi ly b fi mh me l mf mg"><em class="ll">#define the loss function <br/></em>losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><span id="0f10" class="mc jy hi ly b fi mh me l mf mg">#build the model<br/>model.compile(optimizer=optimizerr,<br/>              loss=losss,<br/>              metrics=['accuracy'])</span><span id="b811" class="mc jy hi ly b fi mh me l mf mg"># train the model <br/>model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),<br/>          epochs=N_EPOCHS,<br/>          batch_size=BATCH_SIZE)</span></pre><p id="1db4" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">ä¸‹é¢ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹æœ‰å¤šç²¾ç¡®ã€‚åœ¨ç¬¬äºŒä¸ªæ—¶æœŸï¼Œæˆ‘ä»¬å·²ç»è·å¾—äº†100%çš„å‡†ç¡®åº¦:</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="7a21" class="mc jy hi ly b fi md me l mf mg">&gt;&gt;&gt; Epoch 1/3<br/>&gt;&gt;&gt; 244/244 [==============================] - 131s 374ms/step - <br/>&gt;&gt;&gt; loss: 0.1468 - accuracy: 0.9568<br/>&gt;&gt;&gt; Epoch 2/3<br/>&gt;&gt;&gt; 244/244 [==============================] - 95s 388ms/step - <br/>&gt;&gt;&gt; loss: 3.1370e-04 - accuracy: 1.0000<br/>&gt;&gt;&gt; Epoch 3/3<br/>&gt;&gt;&gt; 244/244 [==============================] - 97s 396ms/step -<br/>&gt;&gt;&gt; loss: 5.7763e-05 - accuracy: 1.0000</span></pre><h1 id="559f" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">æ¨¡å‹è¯„ä¼°ã€‚</h1><p id="61de" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">Tensorflowçš„æ‹¥æŠ±è„¸APIå¯¹ä»»ä½•æ•°æ®ç§‘å­¦å®¶æ–¹æ³•éƒ½å…·æœ‰ç›´è§‚æ€§ã€‚è®©æˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¹¶åœ¨æ–°æ•°æ®å‡ºç°ä¹‹å‰è¿›è¡Œè¯„ä¼°:</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="42c1" class="mc jy hi ly b fi md me l mf mg"><em class="ll"># model evaluation on the test set<br/></em>model.evaluate(test_dataset.shuffle(len(X_test)).batch(BATCH_SIZE), <br/>               return_dict=True, <br/>               batch_size=BATCH_SIZE)</span><span id="c088" class="mc jy hi ly b fi mh me l mf mg"><br/>&gt;&gt;&gt; 61/61 [==============================] - 10s 147ms/step - <br/>&gt;&gt;&gt; loss: 1.7124e-05 - accuracy: 1.0000<br/>&gt;&gt;&gt; {'accuracy': 1.0, 'loss': 1.7123966244980693e-05}</span></pre><p id="1505" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">æˆ‘ä»¬å¾—åˆ°äº†ç›¸å½“å¥½çš„ç»“æœï¼ç°åœ¨ï¼Œå¯¹äºå…¶ä»–æ–‡æœ¬æ®µè½çš„æ¨¡å‹ä¼°è®¡ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æŸ¥çœ‹æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡(ä»¥æŸ¥çœ‹æˆ‘ä»¬çš„æ¨¡å‹åœ¨é¢„æµ‹ä¸­æœ‰å¤šç¡®å®š):</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="5906" class="mc jy hi ly b fi md me l mf mg">def predict_proba(text_list, model, tokenizer):  </span><span id="0bea" class="mc jy hi ly b fi mh me l mf mg"><em class="ll">    #tokenize the text<br/>    </em>encodings = tokenizer(text_list, <br/>                          max_length=MAX_LEN, <br/>                          truncation=True, <br/>                          padding=True)<br/><em class="ll">    #transform to tf.Dataset<br/></em>    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))</span><span id="e936" class="mc jy hi ly b fi mh me l mf mg">    #predict<br/>    preds = model.predict(dataset.batch(1)).logits  <br/>    <br/>    #transform to array with probabilities<br/>    res = tf.nn.softmax(preds, axis=1).numpy()      <br/>    <br/>    return res</span></pre><p id="5488" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">æˆ‘ä»¬è¿™é‡Œå–ä¸€ä¸ª. txtæ–‡ä»¶<a class="ae jw" href="https://github.com/Galina-Blokh/ai_assignment_aidock/blob/refator/data/test_links.txt" rel="noopener ugc nofollow" target="_blank"/>ã€‚è¿™ä¸ªæ–‡ä»¶åŒ…å«10ä¸ªé£Ÿè°±é¡µé¢çš„10ä¸ªURLã€‚æˆ‘ä»¬çš„æ¨¡å‹è¿˜æ²¡æœ‰çœ‹åˆ°æ¥è‡ªå®ƒä»¬çš„æ–‡æœ¬æ•°æ®ã€‚å‡è®¾æ‚¨ä»ç¬¬ä¸€ä¸ª<a class="ae jw" href="https://www.loveandlemons.com/green-bean-salad-recipe/" rel="noopener ugc nofollow" target="_blank"> URL </a>è·å–æ•°æ®ã€‚æ‚¨è¾“å…¥åˆ°é¢„æµ‹æ¨¡å‹ä¸­çš„å­—ç¬¦ä¸²åˆ—è¡¨å°†ç±»ä¼¼äºä¸‹é¢çš„å•å…ƒæ ¼ã€‚(*ç¬¬ä¸€ä¸²æ˜¯é…æ–™ï¼Œåé¢ä¸‰ä¸²æ˜¯è¯´æ˜):</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="60d9" class="mc jy hi ly b fi md me l mf mg">strings_list =["""<br/>                  1 pound green beans, trimmed<br/>                  Â½ head radicchio, sliced into strips<br/>                  Scant Â¼ cup thinly sliced red onion<br/>                  Honey Mustard Dressing, for drizzling<br/>                  2 ounces goat cheese<br/>                  2 tablespoons chopped walnuts<br/>                  2 tablespoons sliced almonds<br/>                  Â¼ cup tarragon<br/>                  Flaky sea salt""",</span><span id="b8d9" class="mc jy hi ly b fi mh me l mf mg">               """Bring a large pot of salted water to a boil and                  set a bowl of ice water nearby. Drop the green beans into the boiling water and blanch for 2 minutes. Remove the beans and immediately immerse in the ice water long enough to cool completely, about 15 seconds. Drain and place on paper towels to dry. """,</span><span id="063e" class="mc jy hi ly b fi mh me l mf mg">               """Transfer the beans to a bowl and toss with the radicchio, onion, and a few spoonfuls of the dressing.""",</span><span id="b8b7" class="mc jy hi ly b fi mh me l mf mg">               """Arrange on a platter and top with small dollops of goat cheese, the walnuts, almonds, and tarragon. Drizzle with more dressing, season to taste with flaky salt, and serve."""]</span></pre><p id="d9ef" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">å½“æ‚¨ä¸ºæ–°æ•°æ®è°ƒç”¨predict_proba()å‡½æ•°æ—¶ï¼Œç»“æœå°†æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º(4ï¼Œ2)çš„NumPyæ•°ç»„ã€‚å››ä¸ªnæ•°ç»„(æ¯ä¸ªæ®µè½ä¸€ä¸ª)ï¼Œå…·æœ‰ä¸¤ä¸ªæ¦‚ç‡å€¼(å¯¹äºç±»0å’Œç±»1):</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="f137" class="mc jy hi ly b fi md me l mf mg">predict_proba(string1, model, tokenizer)</span><span id="f631" class="mc jy hi ly b fi mh me l mf mg">&gt;&gt;&gt; array([<br/>&gt;&gt;&gt; [1.63417135e-05, 9.99983668e-01],<br/>&gt;&gt;&gt; [9.99986053e-01, 1.39580325e-05],<br/>&gt;&gt;&gt; [9.99986053e-01, 1.39833473e-05],        <br/>&gt;&gt;&gt; [9.99988914e-01, 1.11078716e-05]], dtype=float32)</span></pre><p id="3fc7" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">è¯¥æ¨¡å‹çš„é¢„æµ‹éå¸¸å‡†ç¡®ã€‚</p><h1 id="f4de" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">ç»“è®ºã€‚</h1><p id="65e9" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨äº†è§£äº†å¦‚ä½•åœ¨å¸¦æœ‰è‡ªå®šä¹‰å°æ–‡æœ¬æ•°æ®çš„äºŒè¿›åˆ¶åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯¹DistilBert(æ¥è‡ªHugging Face Transformersåº“çš„é¢„è®­ç»ƒæ¨¡å‹)åŠå…¶tensor flow APIè¿›è¡Œå¾®è°ƒã€‚</p><p id="289e" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated"><em class="ll">æœ¬æ–‡çš„</em><a class="ae jw" href="https://colab.research.google.com/drive/14PtqRbrGa70M5NJnLfSllyJYyvl1nYnK?usp=sharing" rel="noopener ugc nofollow" target="_blank"><em class="ll">Google collab</em></a><em class="ll">ç¬”è®°æœ¬ã€‚</em></p><p id="9a1c" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated"><em class="ll">ä¸€ä¸ª</em> <a class="ae jw" href="https://huggingface.co/transformers/master/community.html#community-notebooks" rel="noopener ugc nofollow" target="_blank"> <em class="ll">æŠ±è„¸æ–‡æ¡£é¡µé¢</em> </a> <em class="ll">ï¼Œå…¶ä¸­é‡æ–°ç»„åˆäº†å„åœ°çš„èµ„æºğŸ¤—ç¤¾åŒºå¼€å‘çš„å˜å½¢é‡‘åˆš</em>ã€‚</p></div></div>    
</body>
</html>