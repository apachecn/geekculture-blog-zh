# SerpApi æ¼”ç¤ºé¡¹ç›®:æ²ƒå°”ç›å’–å•¡æ¢ç´¢æ€§æ•°æ®åˆ†æ

> åŸæ–‡ï¼š<https://medium.com/geekculture/serpapi-demo-project-walmart-coffee-exploratory-data-analysis-315908eb1003?source=collection_archive---------13----------------------->

![](img/538e29b5a20bdfdf7ad5e9695726efa6.png)

# ä»‹ç»

è¿™ä¸ª Python æ¼”ç¤ºé¡¹ç›®å®é™…å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ SerpApi çš„[æ²ƒå°”ç›æœç´¢å¼•æ“ç»“æœ API](https://serpapi.com/walmart-search-api) ä»¥åŠå¦‚ä½•å°†æå–çš„æ•°æ®ç”¨äºæ¢ç´¢æ€§æ•°æ®åˆ†æã€‚

SerpApi æ˜¯ä¸€ä¸ª Apiï¼Œè®©ä½ ä» [26 ä¸ªæœç´¢å¼•æ“](https://serpapi.com/search-api)ä»¥ [10 ç§è¯­è¨€](https://serpapi.com/integrations)æ¥æ”¶ç»“æ„åŒ–çš„ JSON æ•°æ®ã€‚

ä½ å¯ä»¥åœ¨ GitHub ä¸Šè‡ªå·±æ¢ç´¢è¿™ä¸ªé¡¹ç›®:`[dimitryzub/walmart-stores-coffee-analysis](https://github.com/dimitryzub/walmart-stores-coffee-analysis)`

æˆ‘ä»¬å°†æ¶µç›–:

1.  ä»æ²ƒå°”ç›æœ‰æœºç»“æœä¸­æå–æ•°æ®ã€‚
2.  ä½¿ç”¨ SerpApi åˆ†é¡µä»æ‰€æœ‰å•†åº—é¡µé¢æå–æ•°æ®ã€‚
3.  ä» 500 å®¶æ²ƒå°”ç›å•†åº—æå–æ•°æ®ã€‚ [SerpApi å‘ JSON Walmart å•†åº—ä½ç½®](https://serpapi.com/walmart-stores)æä¾›æ€»å…± 4.460 å®¶å•†åº—ã€‚
4.  æˆ‘æ¢ç´¢æ€§æ•°æ®åˆ†æçš„å…¨è¿‡ç¨‹ã€‚

ğŸ“Œæ³¨æ„:ä¸‹é¢çš„è„šæœ¬ä»¥ä¸€ç§`sync`çš„æ–¹å¼æå–æ•°æ®ï¼Œè¿™ç§æ–¹å¼æ¯”è¾ƒæ…¢ã€‚

SerpApi æä¾›äº†`[async](https://github.com/serpapi/google-search-results-python#batch-asynchronous-searches)` [å‚æ•°](https://github.com/serpapi/google-search-results-python#batch-asynchronous-searches)ï¼Œå…è®¸å¤„ç†æ›´å¤šçš„ç»“æœï¼Œè€Œæ— éœ€ç­‰å¾… SerpApi å“åº”ã€‚è¿™äº›ç»“æœå°†åœ¨ SerpApi åç«¯è¿›è¡Œå¤„ç†ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ 31 å¤©å†…æ£€ç´¢åˆ°[ã€‚](https://serpapi.com/search-archive-api)

å¦‚æœä½ å¯¹æå–ä¸åˆ° 5 å°æ—¶çš„å¤§é‡æ•°æ®(10.000 å¤šä¸ªè¯·æ±‚)æ„Ÿå…´è¶£ï¼Œ[æ³¨å†Œæˆ‘ä»¬çš„åšå®¢](https://serpapi.com/blog/#/portal/signup/free)ï¼Œè¿™æ ·ä½ å°±ä¸ä¼šé”™è¿‡äº†ã€‚

# æå–æ²ƒå°”ç›æ•°æ®

# è¯´æ˜

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…åº“:

```
$ pip install pandas matplotlib seaborn jinja2 google-search-results
```

*   `[pandas](https://pandas.pydata.org/)`å¼ºå¤§çš„æ•°æ®åˆ†ææ“çºµå·¥å…·ã€‚
*   `[matplotlib](https://matplotlib.org/stable/)Matplotlib`æ˜¯ä¸€ä¸ªç”¨äºå¯è§†åŒ–æ•°æ®çš„ç»¼åˆ Python åº“ã€‚
*   `[seaborn](https://seaborn.pydata.org/)`ä¸€ä¸ªå»ºç«‹åœ¨`matplotlib`ä¹‹ä¸Šçš„é«˜çº§ç•Œé¢ï¼Œç”¨äºç»˜åˆ¶æœ‰å¸å¼•åŠ›å’Œä¿¡æ¯ä¸°å¯Œçš„ç»Ÿè®¡å›¾å½¢ã€‚
*   `[jinja2](https://jinja.palletsprojects.com/en/3.1.x/)` Python æ¨¡æ¿å¼•æ“ã€‚ç”¨äº`pandas`å·¥ä½œå°å³ã€‚
*   `[google-search-results](https://github.com/serpapi/google-search-results-python)` SerpApi Python API åŒ…è£…å™¨ã€‚

å…¶æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥åº“

```
from serpapi import WalmartSearch
from urllib.parse import (parse_qsl, urlsplit)
import pandas as pd
import os, re, json
```

*   `[urllib.parse](https://docs.python.org/3/library/urllib.parse.html)`ç”¨äº SerpApi åˆ†é¡µç›®çš„ã€‚
*   `[os](https://docs.python.org/3/library/os.html)`è¯»å–ç§˜å¯† API å¯†é’¥ã€‚ä¸è¦å…¬å¼€å±•ç¤ºä½ çš„ API å¯†åŒ™ã€‚
*   `[re](https://docs.python.org/3/library/re.html)`ä»`string`ä¸­æå–éƒ¨åˆ†æ•°æ®ã€‚
*   `[json](https://docs.python.org/3/library/json.html)`å¤§éƒ¨åˆ†ç”¨äºæ¼‚äº®å°åˆ·ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯»å–æ²ƒå°”ç›å•†åº—çš„ JSON æ–‡ä»¶:

```
# to get store ID
store_ids = pd.read_json('<path_to>/walmart-stores.json')
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå’–å•¡ç±»å‹çš„å¸¸é‡å˜é‡ã€‚è¿™å°†ç”¨äºç¨åä»åˆ—è¡¨çš„æ ‡é¢˜ä¸­æ£€æŸ¥å’–å•¡ç±»å‹:

```
COFFEE_TYPES = [
    'Espresso',
    '...',
    'Black Ivory Coffee'
]
```

ä¹‹åï¼Œæˆ‘ä»¬è¿­ä»£`store_ids`å¹¶å°†å•†åº— id ä¼ é€’ç»™`[store_id](https://serpapi.com/walmart-search-api#api-parameters-advanced-filters-store-id)` [SerpApi æŸ¥è¯¢å‚æ•°](https://serpapi.com/walmart-search-api#api-parameters-advanced-filters-store-id):

```
for store_id in store_ids['store_id']:
    params = {
        'api_key': os.getenv('API_KEY'),  # serpapi api key
        'engine': 'walmart',              # search engine
        'device': 'desktop',              # device type
        'query': 'coffee',                # search query
        'store_id': store_id
    } search = WalmartSearch(params)       # where data extraction happens
```

åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª`list`æ¥ä¸´æ—¶å­˜å‚¨æå–çš„æ•°æ®ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªä»…ç”¨äºæ¼”ç¤ºçš„é¡µç è®¡æ•°å™¨:

```
data = []
page_num = 0
```

ä¸‹ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª`while`å¾ªç¯ï¼Œå®ƒå°†éå†æ‰€æœ‰å•†åº—é¡µé¢:

```
while True:
    results = search.get_json()      # JSON -> Python dict
```

ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥ SerpApi æ˜¯å¦è¿”å›äº†ä»»ä½•é”™è¯¯ï¼Œå¦‚æœæœ‰ï¼Œåˆ™æ£€æŸ¥`break`:

```
if results['search_information']['organic_results_state'] == 'Fully empty':
    print(results['error'])
    break
```

å¢åŠ ä¸€ä¸ªé¡µé¢è®¡æ•°å™¨å¹¶æ‰“å°å‡ºæ¥:

```
page_num += 1
print(f'Current page: {page_num}')
```

ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦æå–æ•°æ®ï¼Œå¹¶å°½å¯èƒ½å°†å…¶æ ‡å‡†åŒ–:

```
for result in results.get('organic_results', []):
    title = result.get('title').lower() # https://regex101.com/r/h0jTPG/1
    try:
        weight = re.search(r'(\d+-[Ounce|ounce]+|\d{1,3}\.?\d{1,2}[\s|-]?[ounce|oz]+)', title).group(1)
    except: weight = 'not mentioned' # ounce -> grams
    # formula: https://www.omnicalculator.com/conversion/g-to-oz#how-to-convert-grams-to-ounces
    # https://regex101.com/r/wWMUQd/1
    if weight != 'not mentioned':
        weight_formatted_to_gramms = round(float(re.sub(r'[^\d.]+', '', weight)) * 28.34952/1, 1) # loop through COFFEE_TYPES and checks if result.title has coffee type
    # found match or matches will be stored as a list with ',' separator
    coffee_type = ','.join([i for i in COFFEE_TYPES if i.lower() in title])
```

ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®è¿½åŠ åˆ°ä¸´æ—¶çš„`list`:

```
data.append({
    'title': title,
    'coffee_type': coffee_type.lower(),
    'rating': result.get('rating'),
    'reviews': result.get('reviews'),
    'seller_name': result.get('seller_name').lower() if result.get('seller_name') else result.get('seller_name'),
    'thumbnail': result.get('thumbnail'),
    'price': result.get('primary_offer').get('offer_price'),
    'weight': weight,
    'weight_formatted_to_gramms': weight_formatted_to_gramms
})
```

ç„¶åæ£€æŸ¥`serpapi_pagination`å“ˆå¸Œé”®ä¸­æ˜¯å¦æœ‰`next`é¡µå¹¶åˆ†é¡µï¼Œå¦åˆ™`break``while`å¾ªç¯:

```
# check if the next page key is present in the JSON
# if present -> split URL in parts and update to the next page
if 'next' in results.get('serpapi_pagination', {}):
    search.params_dict.update(dict(parse_qsl(urlsplit(results.get('serpapi_pagination', {}).get('next')).query)))
else:
    break
```

æœ€åï¼Œåœ¨æ‰€æœ‰å¾ªç¯å®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦ä¿å­˜æ•°æ®ã€‚æˆ‘ç”¨`pandas` `[to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)`å’Œ`[to_json](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html)`æ–¹æ³•é€‰æ‹© JSON å’Œ CSV:

```
pd.DataFrame(data=data).to_json('coffee-listings-from-all-walmart-stores.json', orient='records')
pd.DataFrame(data=data).to_csv('coffee-listings-from-all-walmart-stores.csv', index=False)
```

# å®Œæ•´ä»£ç 

åœ¨çº¿ IDE (Replit)ä¸­ä¹Ÿæœ‰ä¸€ä¸ª[ä»£ç ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥éšæ„ä½¿ç”¨:](https://replit.com/@DimitryZub1/Scrape-Walmart-Listings-from-Walmart-Stores-with-Pagination#main.py)

```
from serpapi import WalmartSearch
from urllib.parse import (parse_qsl, urlsplit)
import pandas as pd
import os, re, json store_ids = pd.read_json('<path_to>/walmart-stores.json')COFFEE_TYPES = [
    'Espresso',
    'Double Espresso',
    'Red Eye',
    'caramel flavored',
    'caramel',
    'colombian',
    'french',
    'italian',
    'Black Eye',
    'Americano',
    'Long Black',
    'Macchiato',
    'Long Macchiato',
    'Cortado',
    'Breve',
    'Cappuccino',
    'Flat White',
    'Cafe Latte',
    'Mocha',
    'Vienna',
    'Affogato',
    'gourmet coffee',
    'Cafe au Lait',
    'Iced Coffee',
    'Drip Coffee',
    'French Press',
    'Cold Brew Coffee',
    'Pour Over Coffee',
    'Cowboy Coffee',
    'Turkish Coffee',
    'Percolated Coffee',
    'Infused Coffee',
    'Vacuum Coffee',
    'Moka Pot Coffee',
    'Espresso Coffee',
    'Antoccino',
    'Cafe Bombon',
    'Latte',
    'City roast',
    'American roast',
    'Half City roast',
    'New England roast',
    'Light City roast',
    'Blonde roast',
    'Cinnamon roast',
    'Breakfast roast',
    'Full City roast',
    'Continental roast',
    'High roast',
    'New Orleans roast',
    'Espresso roast',
    'Viennese roast',
    'European roast',
    'French roast',
    'Italian roast',
    'Galao',
    'Caffe Americano',
    'Cafe Cubano',
    'Cafe Zorro',
    'Doppio',
    'Espresso Romano',
    'Guillermo',
    'Ristretto',
    'Cafe au lait ',
    'Coffee with Espresso',
    'Dead eye',
    'Botz',
    'Nitro Coffee',
    'Bulletproof Coffee',
    'Black tie',
    'Red tie',
    'Dirty chai latte',
    'Yuenyeung',
    "Bailey's Irish Cream and Coffee",
    'CaffÃ¨ Corretto',
    'RÃ¼desheimer Kaffee',
    'Pharisee',
    'Barraquito',
    'Carajillo',
    'Irish coffee',
    'Melya',
    'Espressino',
    'CaffÃ¨ Marocchino',
    'CafÃ© miel',
    'Cafe Borgia',
    'CafÃ© de olla',
    'CafÃ© RÃ¡pido y Sucio',
    'Coffee with a flavor shot',
    'CaffÃ¨ Medici',
    'Egg coffee',
    'Kopi susu',
    'Vienna Coffee',
    'Iced lattes',
    'Iced mochas',
    'Ca phe sua da',
    'Eiskaffee',
    'FrappÃ©',
    'Freddo Espresso',
    'Freddo Cappuccino',
    'Mazagran',
    'Palazzo',
    'Ice Shot',
    'Shakerato',
    'Instant Coffee',
    'Canned Coffee',
    'Coffee Milk',
    'South Indian Coffee',
    'Pocillo',
    'Arabica',
    'Robusta Beans',
    'Liberica Beans',
    'Excelsa Beans',
    'White Roast Coffee',
    'Light Roast',
    'Medium Roast',
    'Medium-Dark Roast',
    'medium dark',
    'medium dark roast',
    'Classic Roast',
    'black silk ground coffee',
    'black rifle coffee',
    'Dark Roast',
    'Kopi Luwak Coffee',
    'Jacu Bird Coffee',
    'Black Ivory Coffee'
]for store_id in store_ids['store_id']:
    params = {
        'api_key': os.getenv('API_KEY'),  # serpapi api key
        'engine': 'walmart',              # search engine
        'device': 'desktop',              # device type
        'query': 'coffee',                # search query
        'store_id': store_id
    } search = WalmartSearch(params)       # where data extraction happens
    print(params)                        # just to show the progress data = []
    page_num = 0 while True:
        results = search.get_json()      # JSON -> Python dict if results['search_information']['organic_results_state'] == 'Fully empty':
            print(results['error'])
            break page_num += 1
        print(f'Current page: {page_num}') for result in results.get('organic_results', []):
            title = result.get('title').lower() if result.get('title') else result.get('title') # https://regex101.com/r/h0jTPG/1
            try:
                weight = re.search(r'(\d+-[Ounce|ounce]+|\d{1,3}\.?\d{1,2}[\s|-]?[ounce|oz]+)', title).group(1)
            except: weight = 'not mentioned' # ounce -> grams
            # formula: https://www.omnicalculator.com/conversion/g-to-oz#how-to-convert-grams-to-ounces
            # https://regex101.com/r/wWMUQd/1
            if weight != 'not mentioned':
                weight_formatted_to_gramms = round(float(re.sub(r'[^\d.]+', "", weight)) * 28.34952/1, 1) coffee_type = ",".join([i for i in COFFEE_TYPES if i.lower() in title]) data.append({
                'title': title,
                'coffee_type': coffee_type.lower(),
                'rating': result.get('rating'),
                'reviews': result.get('reviews'),
                'seller_name': result.get('seller_name').lower() if result.get('seller_name') else result.get('seller_name'),
                'thumbnail': result.get('thumbnail'),
                'price': result.get('primary_offer').get('offer_price'),
                'weight': weight,
                'weight_formatted_to_gramms': weight_formatted_to_gramms
            }) # check if the next page key is present in the JSON
        # if present -> split URL in parts and update to the next page
        if 'next' in results.get('serpapi_pagination', {}):
            search.params_dict.update(dict(parse_qsl(urlsplit(results.get('serpapi_pagination', {}).get('next')).query)))
        else:
            breakpd.DataFrame(data=data).to_json('coffee-listings-from-all-walmart-stores.json', orient='records')
pd.DataFrame(data=data).to_csv('coffee-listings-from-all-walmart-stores.csv', index=False)
```

# æ¢ç´¢æ€§åˆ†æ

# å…³é”®è¦ç‚¹

1.  æœ€å—æ¬¢è¿çš„å’–å•¡é”€å”®å•†æ˜¯æ²ƒå°”ç›ã€‚
2.  æœ€å—æ¬¢è¿çš„å’–å•¡ç±»å‹æ˜¯ä¸­åº¦çƒ˜ç„™ã€‚
3.  æ›´å¤šçš„é‡é‡(å…‹)å¹¶ä¸ç­‰äºæ›´é«˜çš„ä»·æ ¼ã€‚

*   ä½å…‹å’–å•¡å¯èƒ½æ¯”é«˜å…‹å’–å•¡è´µã€‚

1.  æœ€é«˜çš„å’–å•¡é‡é‡æ˜¯ 2835 å…‹(2.8 å…¬æ–¤)ã€‚
2.  â€œFolgers classic roast ground coffeeâ€æœ‰ 15k+è¯„è®ºï¼Œè¿™æ˜¯æ•°æ®é›†çš„æœ€å¤§å€¼ã€‚
3.  å¤§çº¦ 300-500 å…‹æ˜¯æœ€å¸¸è§çš„é‡é‡ã€‚
4.  æœ€é«˜çš„å’–å•¡ä»·æ ¼æ˜¯ 77 ç¾å…ƒ(Lavazza perfetto å•æ¯ k æ¯)

# è¦å›ç­”çš„é—®é¢˜

æˆ‘åœ¨æ¢ç´¢ä¹‹åˆå°±å†™ä¸‹äº†é‚£äº›é—®é¢˜ï¼Œä»¥è·Ÿè¸ªä»–ä»¬çš„è¿›å±•ã€‚è¯·è®°ä½ï¼Œè¿™äº›é—®é¢˜åªåæ˜ äº†æˆ‘çš„ä¸ªäººå…´è¶£ã€‚

*   ä»€ä¹ˆå’–å•¡æ ‡é¢˜è¯„è®ºæœ€å¤šï¼Ÿ
*   ä»€ä¹ˆå’–å•¡åæœ€æœ‰æ”¶è§†ç‡ï¼Ÿ
*   æœ€å—æ¬¢è¿çš„ç•…é”€ä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ
*   ä»€ä¹ˆå’–å•¡åçš„ä»·æ ¼æœ€é«˜/æœ€ä½ï¼Ÿ
*   æ€»é‡é‡æ˜¯å¤šå°‘å…‹ï¼Ÿ
*   ä»€ä¹ˆå’–å•¡çš„é‡é‡æœ€é«˜/æœ€ä½(å…‹)ï¼Ÿ
*   æœ€å—æ¬¢è¿çš„å’–å•¡ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ
*   æœ€å¸¸å–çš„å’–å•¡å…‹æ•°ï¼Ÿ
*   æ€»é‡é‡æ˜¯å¤šå°‘å…‹ï¼Ÿ
*   æ›´é«˜çš„é‡é‡(å…‹)=æ›´é«˜çš„ä»·æ ¼ï¼Ÿ
*   æ›´ä½çš„é‡é‡(å…‹)=æ›´ä½çš„ä»·æ ¼ï¼Ÿ

# è¿‡ç¨‹

å®‰è£…åº“å¹¶å‘Šè¯‰`[matplotlib](https://matplotlib.org/stable/)`åœ¨`[%](https://ipython.readthedocs.io/en/stable/interactive/tutorial.html#magics-explained)` [ç¥å¥‡å‡½æ•°](https://ipython.readthedocs.io/en/stable/interactive/tutorial.html#magics-explained)çš„å¸®åŠ©ä¸‹å†…è”ç»˜å›¾(åœ¨ç¬”è®°æœ¬å†…éƒ¨)ï¼Œè¯¥å‡½æ•°å°†`matplotlib`çš„åç«¯è®¾ç½®ä¸º`inline`åç«¯:

```
%pip install pandas matplotlib seaborn jinja2 # install libraries
%matplotlib inline
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¼å…¥ä¹‹å‰ä½¿ç”¨[æ²ƒå°”ç›æœç´¢å¼•æ“ç»“æœ API](https://serpapi.com/walmart-search-api) ä» SerpApi ä¸­æå–çš„`[pandas](https://pandas.pydata.org/)`å’Œ`[read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)`:

```
import pandas as pdcoffee_df = pd.read_csv('/workspace/serpapi-demo-projects/walmart-coffee-analysis/data/coffee-listings-from-all-walmart-stores.csv')
coffee_df.head()
```

![](img/dfe9e18edef186ed7d5e794bbe738013.png)

ç°åœ¨æˆ‘æƒ³æ£€æŸ¥ä¸€ä¸‹å…³äº`[DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)`çš„å…¨éƒ¨ä¿¡æ¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æ­£åœ¨æŸ¥çœ‹æ¯ä¸€åˆ—çš„`Dtype`ï¼Œå¦‚æœå®ƒæ˜¯æ­£ç¡®çš„:

```
coffee_df.info()
```

äº§å‡º:

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1400 entries, 0 to 1399
Data columns (total 9 columns):
  #   Column                      Non-Null Count  Dtype  
---  ------                      --------------  -----  
  0   title                       1400 non-null   object 
  1   coffee_type                 1121 non-null   object 
  2   rating                      1400 non-null   float64
  3   reviews                     1400 non-null   int64  
  4   seller_name                 1400 non-null   object 
  5   thumbnail                   1400 non-null   object 
  6   price                       1400 non-null   float64
  7   weight                      1400 non-null   object 
  8   weight_formatted_to_gramms  1400 non-null   float64
dtypes: float64(3), int64(1), object(5)
memory usage: 98.6+ KB
```

æ¥ä¸‹æ¥ï¼Œæˆ‘å¸Œæœ›çœ‹åˆ°æ•´ä½“æ•°å­—ä¿¡æ¯ï¼Œå¦‚å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ï¼Œä»¥åŠå®ƒä»¬æ˜¯å¦æœ‰æ„ä¹‰:

```
coffee_df.describe()
```

![](img/965744e6b605ba879791b47746e728d7.png)

```
coffee_df.shape# (1400, 9)
```

è¿™é‡Œæˆ‘æƒ³ä½¿ç”¨`[DataFrame.corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)`å¿«é€Ÿæµè§ˆä¸€ä¸‹æ•°å­—åˆ—ä¹‹é—´çš„ç›¸å…³æ€§:

```
import seaborn as sns
import matplotlib as pltax = sns.heatmap(coffee_df.corr(numeric_only=True), annot=True, cmap='Blues')
ax.set_title('Correlation between variables')
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å’–å•¡çš„é‡é‡å’Œä»·æ ¼ä¹‹é—´æœ‰å¾ˆå°çš„å…³è”ï¼Œè¿™å¾ˆç¬¦åˆé€»è¾‘:

![](img/81dff28159eefb357e57c851aaa7f3ca.png)

## ä»€ä¹ˆå’–å•¡æ ‡é¢˜è¯„è®ºæœ€å¤šï¼Ÿ

ç­”æ¡ˆ:ç¦å°”æ°ç»å…¸çƒ˜ç„™ç ”ç£¨å’–å•¡ï¼Œä¸­ç­‰ roâ€¦ / 15148 è¯„è®º

## ä»€ä¹ˆå’–å•¡åæœ€æœ‰æ”¶è§†ç‡ï¼Ÿ

ç­”æ¡ˆ:ç¤¾åŒºå’–å•¡é¦†ç‰¹æ®Šè„±å’–å•¡å› ä¸­æ·±çƒ˜ç„™å’–å•¡å•æ¯ 36 ct ç›’è£…å…¼å®¹ keurig 2.0 k æ¯å•¤é…’/5 çº§å’Œ 108 çº§è¯„è®º

```
coffee_df.query('reviews == reviews.max()')[['title', 'reviews']]
```

![](img/c818b5c142552d590d89be1ae1a89892.png)

```
coffee_df.query('rating == rating.max()')[['title', 'rating', 'reviews']].sort_values(
    by='reviews', ascending=False
).style.hide(axis='index').background_gradient(cmap='Blues')
```

## æœ€å—æ¬¢è¿çš„ç•…é”€ä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ

ç­”æ¡ˆ:æ²ƒå°”ç›

```
plt.title('Most Popular Coffee Seller on Walmart')# value_counts() to count how many times each seller is repeated
# head() display only top 10
# sort_values() to sort from highest to lowest
# and plot the dataax = coffee_df['seller_name'].value_counts().head(10).sort_values().plot(kind='barh', figsize=(13,5))
ax.bar_label(ax.containers[0])plt.xlabel('Number of listings')
plt.show()
```

![](img/c4a4a27cf0c5c9459d8dc8f2b287765c.png)

## æ€»é‡é‡æ˜¯å¤šå°‘å…‹ï¼Ÿ

ç­”æ¡ˆ:869948.5 å…‹ï¼Œä¹Ÿå°±æ˜¯ 870 å…¬æ–¤ğŸ¤“

```
# sum of the all grams
grams = coffee_df['weight_formatted_to_gramms'].sum()
kilorgrams = round(coffee_df['weight_formatted_to_gramms'].sum() / 1000)print(f'Grams: {grams}\nKilograms: {kilorgrams}')Grams: 869948.5
Kilograms: 870
```

## ä»€ä¹ˆå’–å•¡åçš„ä»·æ ¼æœ€é«˜/æœ€ä½ï¼Ÿ

å›ç­”æœ€é«˜:lavazza perfetto keurig brewer å•æ¯ k æ¯å’–å•¡ç›’ï¼Œæ·±è‰²çƒ˜ç„™ï¼Œ10 ç›’(æ¯ç›’ 6 ä¸ª)/ä»·æ ¼ 77.09 ç¾å…ƒ

å›ç­”æœ€ä½:

*   folgers ç»å…¸çƒ˜ç„™é€Ÿæº¶å’–å•¡ï¼Œå•ä»½åŒ…è£…/ä»·æ ¼ 1 ç¾å…ƒ
*   ç»å…¸æ— å’–å•¡å› é€Ÿæº¶å’–å•¡æ°´æ™¶åŒ…ï¼Œ6 æ”¯/ä»·æ ¼ 1 ç¾å…ƒ

æœ€é«˜ä»·æ ¼

```
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html
coffee_df.query('price == price.max()')[['title', 'price']]
```

![](img/451cbef894f1d8b91fbe6cbc36136c25.png)

æœ€ä½ä»·æ ¼

*   folgers ç»å…¸çƒ˜ç„™é€Ÿæº¶å’–å•¡ï¼Œå•ä»½åŒ…è£…/ä»·æ ¼ 1 ç¾å…ƒ
*   folgers ç»å…¸æ— å’–å•¡å› é€Ÿæº¶å’–å•¡æ°´æ™¶åŒ…ï¼Œ6 æ”¯/ä»·æ ¼ 1 ç¾å…ƒ

```
# condition with .min() won't work: coffee_df.loc[(coffee_df['price'] != 0) & (coffee_df['price'].min()), ['price']]# != 0 to exclude 0 values
coffee_df.loc[(coffee_df['price'] != 0) & (coffee_df['price'] < 1.1), ['title', 'price']]
```

![](img/30d8315427c7a7e188a473059de74d2d.png)

## ä»€ä¹ˆå’–å•¡çš„é‡é‡æœ€é«˜/æœ€ä½(å…‹)ï¼Ÿ

å›ç­”æœ€é«˜:

![](img/6cfb62c73ae53859ad0f59b69feb4e04.png)

å›ç­”æœ€ä½:

![](img/9e9aecdad121cc97d7eee9a48d9699ab.png)

æœ€é«˜é‡é‡

```
coffee_df.query('weight_formatted_to_gramms == weight_formatted_to_gramms.max()')[
    ['title', 'weight_formatted_to_gramms']
]
```

![](img/e3a7e61428c21771151428f12a618bbb.png)

æœ€ä½é‡é‡

ç”±äºæœªæ­£ç¡®æå–æ•°æ®ï¼Œæœ‰ 7 ä¸ªæŒ‡æ•°çš„å€¼ä¸º 0.0ã€‚

```
coffee_df.query('weight_formatted_to_gramms == weight_formatted_to_gramms.min()')[
    'weight_formatted_to_gramms'
]
```

äº§å‡º:

```
305     0.0
425     0.0
703     0.0
890     0.0
957     0.0
991     0.0
1188    0.0
Name: weight_formatted_to_gramms, dtype: float64
```

æˆ‘å†³å®šæ£€æŸ¥é‡é‡ä¸ç­‰äº`0` `and`é‡é‡ä¸é«˜äº`10`å…‹:

```
coffee_df.loc[
    (coffee_df['weight_formatted_to_gramms'] != 0.0)
    & (coffee_df['weight_formatted_to_gramms'] < 10),
    ['title', 'weight_formatted_to_gramms'],
]
```

![](img/83c50db6264dc4b17cdfc68ab14b1806.png)

## æœ€å—æ¬¢è¿çš„å’–å•¡ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ

ç­”æ¡ˆ(å‰ 3 å):

1.  ä¸­åº¦çƒ˜ç„™
2.  æ·±è‰²çƒ¤è‚‰
3.  é˜¿æ‹‰ä¼¯å’–å•¡

ğŸ“Œæ³¨æ„:æœ‰ 125 ç§ä¸åŒçš„å’–å•¡ï¼Œå…¶ä¸­ä¸€äº›å¯èƒ½ä¼šä¸¢å¤±ã€‚

çœ‹çœ‹`extraction.py`ä¸‹çš„`COFFEE_TYPES`å’Œ`list`

æ‹†åˆ†å’–å•¡ç±»å‹å¹¶æ‰©å±•åˆ°ä¸€ä¸ªæ–°çš„åˆ—è¡¨ä¸­

```
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html
coffee_types = coffee_df['coffee_type'].fillna(method='ffill') # fill with Nonecoffee_types_new = []# split by comma and extend the new list with split values
# https://stackoverflow.com/a/27886807/15164646
for coffee_type in coffee_types:
    coffee_types_new.extend(coffee_type.split(','))
```

è·å–æœ€å¸¸è§çš„å’–å•¡ç±»å‹

```
coffee_types_new_series = pd.Series(coffee_types_new).dropna()plt.title('Most Popular Coffee Types')ax = coffee_types_new_series.value_counts().sort_values(ascending=True).plot(kind='barh', figsize=(15,10))
ax.bar_label(ax.containers[0]) # bar annotationplt.ylabel('Coffee Type')
plt.xlabel('Number of Occurrences')plt.show()
```

![](img/abd6ef6bede7024992ec29626ee7926a.png)

## æœ€å¸¸å–çš„å’–å•¡é‡é‡ï¼Ÿ

ç­”æ¡ˆ:~ 300â€“500 å…‹

```
sns.jointplot(data=coffee_df, x='price', y='weight_formatted_to_gramms', kind='hex')
```

![](img/49f70f233c1a9c392876957c3c693c4c.png)

é™„åŠ å›¾

```
g = sns.PairGrid(coffee_df[['price', 'weight_formatted_to_gramms']], height=4)
g.map_upper(sns.histplot)
g.map_lower(sns.kdeplot, fill=True)
g.map_diag(sns.histplot, kde=True)
```

![](img/fae52b60785788035d3ec6e25f332147.png)

```
sns.displot(
    coffee_df,
    x='price',
    y='weight_formatted_to_gramms',
    kind='kde',
    fill=True,
    thresh=0,
    levels=100,
    cmap='mako',
)
```

![](img/c752a3c4d38e882c59b63e32bb7c389f.png)

## æ›´é«˜çš„é‡é‡(å…‹)=æ›´é«˜çš„ä»·æ ¼ï¼Ÿ

ç­”:è¿™å–å†³äºæ•°æ®é›†ã€‚

## æ›´ä½çš„é‡é‡(å…‹)=æ›´ä½çš„ä»·æ ¼ï¼Ÿ

ç­”:è¿™å–å†³äºæ•°æ®é›†ã€‚

```
sns.jointplot(data=coffee_df, x='price', y='weight_formatted_to_gramms', kind='reg')
```

![](img/2d8888aff67fac643b128ffc5ba9a744.png)

```
ax = sns.scatterplot(
    data=coffee_df,
    x='price',
    y='weight_formatted_to_gramms',
    hue='price',
    size='price',
    sizes=(40, 300),
)# https://stackoverflow.com/a/34579525/15164646
sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1.02)) # 1 = X axis, 1.02 = Y axis of the legend.
```

![](img/36ce7e2449a66e251b526109d8a1db51.png)

# ç»“è®º

åœ¨è¿™ç¯‡åšæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†:

*   [æ²ƒå°”ç›å¼•æ“ç»“æœ API](https://serpapi.com/walmart-search-api) ã€‚
*   [æ²ƒå°”ç›é—¨åº—ä½ç½®çš„ JSON](https://serpapi.com/walmart-stores)ã€‚
*   SerpApi åˆ†é¡µã€‚
*   ç”¨`pandas`ã€`matplotlib`å’Œ`seaborn`è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æã€‚
*   åˆ†ç»„
*   æ•´ç†
*   æµ‹ç»˜

åŠ å…¥æˆ‘ä»¬çš„[Twitter](https://twitter.com/serp_api)|[YouTube](https://www.youtube.com/channel/UCUgIHlYBOD3yA3yDIRhg_mg)

æ·»åŠ ä¸€ä¸ª[ç‰¹å¾è¯·æ±‚](https://github.com/serpapi/public-roadmap/issues)ğŸ’«è¿˜æ˜¯ä¸€ä¸ª [Bug](https://github.com/serpapi/public-roadmap/issues) ğŸ