# æœºå™¨å­¦ä¹ æ•™ç¨‹â€”é¢å‘åˆå­¦è€…çš„ç‰¹å¾å·¥ç¨‹å’Œç‰¹å¾é€‰æ‹©

> åŸæ–‡ï¼š<https://medium.com/geekculture/machine-learning-tutorial-feature-engineering-and-feature-selection-for-beginners-dd15b9d354?source=collection_archive---------8----------------------->

![](img/4e782c2078ce2126fcf617f641164041.png)

ä»–ä»¬è¯´**æ•°æ®**æ˜¯æ–°çš„**çŸ³æ²¹**ï¼Œä½†æ˜¯æˆ‘ä»¬å¹¶ä¸ç›´æ¥ä½¿ç”¨å…¶æ¥æºçš„çŸ³æ²¹ã€‚åœ¨æˆ‘ä»¬å°†å®ƒç”¨äºä¸åŒç›®çš„ä¹‹å‰ï¼Œå®ƒå¿…é¡»ç»è¿‡å¤„ç†å’Œæ¸…æ´—ã€‚
è¿™åŒæ ·é€‚ç”¨äºæ•°æ®ï¼Œæˆ‘ä»¬ä¸ç›´æ¥ä»å®ƒçš„æ¥æºä½¿ç”¨å®ƒã€‚å®ƒä¹Ÿå¿…é¡»è¢«å¤„ç†ã€‚

![](img/1dfc5f21025ff3b677cf3f15cfb631ea.png)

Oil industry

å¯¹äºæœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦çš„åˆå­¦è€…æ¥è¯´ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæ•°æ®æ¥è‡ªä¸åŒçš„æ¥æºï¼Œå…·æœ‰ä¸åŒçš„æ•°æ®ç±»å‹ã€‚å› æ­¤ï¼Œæ‚¨ä¸èƒ½å¯¹ä¸åŒç±»å‹çš„æ•°æ®åº”ç”¨ç›¸åŒçš„æ¸…ç†å’Œå¤„ç†æ–¹æ³•ã€‚

> *â€œå¯ä»¥ä»æ•°æ®ä¸­æå–ä¿¡æ¯ï¼Œå°±åƒå¯ä»¥ä»çŸ³æ²¹ä¸­æå–èƒ½é‡ä¸€æ ·ã€‚â€-* [*é˜¿å¾·å¥¥æ‹‰*](/@adeolaadesina)

ä½ å¿…é¡»æ ¹æ®ä½ æ‰€æŒæ¡çš„æ•°æ®æ¥å­¦ä¹ å’Œè¿ç”¨æ–¹æ³•ã€‚ç„¶åä½ å¯ä»¥ä»ä¸­è·å¾—æ´å¯ŸåŠ›ï¼Œæˆ–è€…ç”¨å®ƒæ¥è¿›è¡Œæœºå™¨å­¦ä¹ æˆ–è€…æ·±åº¦å­¦ä¹ ç®—æ³•çš„è®­ç»ƒã€‚

çœ‹å®Œè¿™ç¯‡æ–‡ç« ï¼Œä½ ä¼šçŸ¥é“:

*   ä»€ä¹ˆæ˜¯ç‰¹å¾å·¥ç¨‹å’Œç‰¹å¾é€‰æ‹©ã€‚
*   å¤„ç†æ•°æ®é›†ä¸­ç¼ºå¤±æ•°æ®çš„ä¸åŒæ–¹æ³•ã€‚
*   å¤„ç†è¿ç»­ç‰¹å¾çš„ä¸åŒæ–¹æ³•ã€‚
*   å¤„ç†åˆ†ç±»ç‰¹å¾çš„ä¸åŒæ–¹æ³•ã€‚
*   ä¸åŒçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚

è®©æˆ‘ä»¬å¼€å§‹å§ã€‚ğŸš€

# ä»€ä¹ˆæ˜¯ç‰¹å¾å·¥ç¨‹ï¼Ÿ

ç‰¹å¾å·¥ç¨‹æ˜¯æŒ‡åœ¨ä½¿ç”¨æœºå™¨å­¦ä¹ åˆ›å»º**é¢„æµ‹æ¨¡å‹**æ—¶ï¼Œåœ¨æ‚¨çš„æ•°æ®é›†ä¸­é€‰æ‹©å¹¶**è½¬æ¢**å˜é‡/ç‰¹å¾çš„è¿‡ç¨‹ã€‚

å› æ­¤ï¼Œåœ¨ç”¨æœºå™¨å­¦ä¹ ç®—æ³•è®­ç»ƒæ•°æ®ä¹‹å‰ï¼Œæ‚¨å¿…é¡»ä»æ‚¨æ”¶é›†çš„**åŸå§‹æ•°æ®é›†**ä¸­æå–ç‰¹å¾ã€‚
å¦åˆ™ï¼Œå¾ˆéš¾è·å¾—å¯¹æ•°æ®çš„æ·±åˆ»è§è§£ã€‚

> *æ‹·é—®æ•°æ®ï¼Œå®ƒä»€ä¹ˆéƒ½ä¼šæ‹›ä¾›ã€‚â€”ç½—çº³å¾·Â·ç§‘æ–¯*

ç‰¹å¾å·¥ç¨‹æœ‰ä¸¤ä¸ªç›®æ ‡:

*   å‡†å¤‡é€‚å½“çš„è¾“å…¥æ•°æ®é›†ï¼Œä¸æœºå™¨å­¦ä¹ ç®—æ³•è¦æ±‚å…¼å®¹ã€‚
*   æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„**æ€§èƒ½**ã€‚

![](img/8cfdc954bbc4ea05cb0b0fd004436402.png)

CrowdFlower Survey

æ ¹æ® CrowdFlower å¯¹ 80 åæ•°æ®ç§‘å­¦å®¶çš„è°ƒæŸ¥ï¼Œæ•°æ®ç§‘å­¦å®¶èŠ±è´¹ **60%** çš„æ—¶é—´æ¸…ç†å’Œç»„ç»‡æ•°æ®ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ‹¥æœ‰ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©çš„æŠ€èƒ½æ˜¯éå¸¸é‡è¦çš„ã€‚

> *â€œå½’æ ¹ç»“åº•ï¼Œæœ‰äº›æœºå™¨å­¦ä¹ é¡¹ç›®æˆåŠŸäº†ï¼Œæœ‰äº›å¤±è´¥äº†ã€‚æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿè½»æ¾æœ€é‡è¦çš„å› ç´ æ˜¯*çš„**ç‰¹æ€§**çš„ä½¿ç”¨åç››é¡¿å¤§å­¦çš„ Pedro Domingos æ•™æˆ

å¯ä»¥ä»ä»¥ä¸‹é“¾æ¥é˜…è¯»ä»–çš„è®ºæ–‡:[ã€Šå…³äºæœºå™¨å­¦ä¹ éœ€è¦çŸ¥é“çš„å‡ ä»¶æœ‰ç”¨çš„äº‹æƒ…](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)ã€‹ã€‚

æ—¢ç„¶æ‚¨çŸ¥é“äº†ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ ä¸åŒçš„ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬ä»å­¦ä¹ å¤„ç†ç¼ºå¤±æ•°æ®çš„ä¸åŒæ–¹æ³•å¼€å§‹ã€‚

# å¦‚ä½•å¤„ç†ä¸¢å¤±çš„æ•°æ®

å¤„ç†ç¼ºå¤±æ•°æ®éå¸¸é‡è¦ï¼Œå› ä¸ºè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•ä¸æ”¯æŒå…·æœ‰ç¼ºå¤±å€¼çš„æ•°æ®ã€‚å¦‚æœæ•°æ®é›†ä¸­æœ‰ç¼ºå¤±å€¼ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸€äº›æœºå™¨å­¦ä¹ ç®—æ³•å‡ºé”™å’Œæ€§èƒ½ä¸‹é™ã€‚

ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸­æ‰¾åˆ°çš„å¸¸è§ç¼ºå¤±å€¼åˆ—è¡¨ã€‚

*   ä¸é€‚ç”¨çš„
*   ç©º
*   ç©ºçš„
*   ?
*   æ²¡æœ‰äºº
*   ç©ºçš„
*   -
*   åœ†ç›˜çƒ¤é¥¼

è®©æˆ‘ä»¬å­¦ä¹ ä¸åŒçš„æ–¹æ³•æ¥è§£å†³ç¼ºå¤±æ•°æ®çš„é—®é¢˜ã€‚

# å˜é‡åˆ é™¤

å˜é‡åˆ é™¤åŒ…æ‹¬æ ¹æ®å…·ä½“æƒ…å†µåˆ é™¤ç¼ºå°‘å€¼çš„å˜é‡(åˆ—)ã€‚å½“å˜é‡ä¸­æœ‰å¾ˆå¤šç¼ºå¤±å€¼ï¼Œå¹¶ä¸”å˜é‡ç›¸å¯¹ä¸å¤ªé‡è¦æ—¶ï¼Œè¿™ç§æ–¹æ³•æ˜¯æœ‰æ„ä¹‰çš„ã€‚

å”¯ä¸€å€¼å¾—åˆ é™¤å˜é‡çš„æƒ…å†µæ˜¯å½“å®ƒçš„ç¼ºå¤±å€¼è¶…è¿‡è§‚å¯Ÿå€¼çš„ 60%æ—¶ã€‚

```
# import packages
import numpy as np 
import pandas as pd 

# read dataset 
data = pd.read_csv('path/to/data')

#set threshold
threshold = 0.7

# dropping columns with missing value rate higher than threshold
data = data[data.columns[data.isnull().mean() < threshold]]
```

åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘å¦‚ä½•ä½¿ç”¨ NumPy å’Œ pandas æ¥åŠ è½½æ•°æ®é›†ï¼Œå¹¶å°†é˜ˆå€¼è®¾ç½®ä¸º **0.7** ã€‚è¿™æ„å‘³ç€ä»»ä½•ç¼ºå¤±å€¼è¶…è¿‡è§‚å¯Ÿå€¼çš„ **70%** çš„åˆ—éƒ½å°†ä»æ•°æ®é›†ä¸­åˆ é™¤ã€‚

æˆ‘å»ºè®®æ‚¨æ ¹æ®æ•°æ®é›†çš„å¤§å°æ¥è®¾ç½®é˜ˆå€¼ã€‚

# å‡å€¼æˆ–ä¸­ä½æ•°æ’è¡¥

å¦ä¸€ç§å¸¸è§çš„æŠ€æœ¯æ˜¯ä½¿ç”¨éç¼ºå¤±è§‚æµ‹å€¼çš„å¹³å‡å€¼æˆ–ä¸­å€¼ã€‚æ­¤ç­–ç•¥å¯åº”ç”¨äºå…·æœ‰æ•°å€¼æ•°æ®çš„è¦ç´ ã€‚

```
# filling missing values with medians of the columns
data = data.fillna(data.median())
```

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨**ä¸­å€¼æ–¹æ³•**æ¥å¡«å……æ•°æ®é›†ä¸­ç¼ºå¤±çš„å€¼ã€‚

# æœ€å¸¸è§çš„å€¼

è¯¥æ–¹æ³•ç”¨åˆ—/ç‰¹å¾ä¸­çš„**æœ€å¤§å‡ºç°å€¼**æ›¿æ¢ç¼ºå¤±å€¼ã€‚è¿™æ˜¯å¤„ç†**åˆ†ç±»**åˆ—/ç‰¹æ€§çš„ä¸€ä¸ªå¥½é€‰æ‹©ã€‚

```
# filling missing values with medians of the columns
data['column_name'].fillna(data['column_name'].value_counts().idxmax(). inplace=True)
```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ pandas çš„ **value_counts()** æ–¹æ³•æ¥è®¡ç®—åˆ—ä¸­æ¯ä¸ªå”¯ä¸€å€¼çš„å‡ºç°æ¬¡æ•°ï¼Œç„¶åç”¨æœ€å¸¸è§çš„å€¼å¡«å……ç¼ºå¤±çš„å€¼ã€‚

# å¦‚ä½•å¤„ç†è¿ç»­ç‰¹å¾

æ•°æ®é›†ä¸­çš„è¿ç»­è¦ç´ å…·æœ‰ä¸åŒçš„å€¼èŒƒå›´ã€‚è¿ç»­ç‰¹å¾çš„å¸¸è§ä¾‹å­æœ‰å¹´é¾„ã€å·¥èµ„ã€ä»·æ ¼å’Œèº«é«˜ã€‚

åœ¨è®­ç»ƒæœºå™¨å­¦ä¹ ç®—æ³•ä¹‹å‰ï¼Œå¤„ç†æ•°æ®é›†ä¸­çš„è¿ç»­ç‰¹å¾éå¸¸é‡è¦ã€‚å¦‚æœæ‚¨ä½¿ç”¨ä¸åŒèŒƒå›´çš„å€¼æ¥è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹å°†ä¸ä¼šå¾ˆå¥½åœ°æ‰§è¡Œã€‚

å½“æˆ‘è¯´ä¸åŒèŒƒå›´çš„å€¼æ—¶ï¼Œæˆ‘æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Ÿå‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå®ƒæœ‰ä¸¤ä¸ªè¿ç»­çš„ç‰¹å¾ï¼Œ**å¹´é¾„ï¼Œ**å’Œ**è–ªæ°´**ã€‚å¹´é¾„çš„èŒƒå›´å’Œè–ªæ°´çš„èŒƒå›´æ˜¯ä¸åŒçš„ï¼Œè¿™å¯èƒ½ä¼šå¼•èµ·é—®é¢˜ã€‚

![](img/6566d19ba9edfe11a51ba61772e80f42.png)

ä»¥ä¸‹æ˜¯å¤„ç†è¿ç»­ç‰¹å¾çš„ä¸€äº›å¸¸ç”¨æ–¹æ³•:

# æœ€å°-æœ€å¤§å½’ä¸€åŒ–

å¯¹äºè¦ç´ ä¸­çš„æ¯ä¸ªå€¼ï¼Œæœ€å°-æœ€å¤§å½’ä¸€åŒ–ä¼šå‡å»è¦ç´ ä¸­çš„æœ€å°å€¼ï¼Œç„¶åé™¤ä»¥å…¶èŒƒå›´ã€‚è¯¥èŒƒå›´æ˜¯åŸå§‹æœ€å¤§å€¼å’ŒåŸå§‹æœ€å°å€¼ä¹‹é—´çš„å·®å€¼ã€‚

![](img/c54af77a0443bb8808017ce53bef0d51.png)

æœ€åï¼Œå®ƒç¼©æ”¾åœ¨ **0** å’Œ **1 ä¹‹é—´çš„å›ºå®šèŒƒå›´å†…çš„æ‰€æœ‰å€¼ã€‚**

æ‚¨å¯ä»¥ä½¿ç”¨ Scikit-learn ä¸­çš„[**minmax scaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)**æ–¹æ³•ï¼Œé€šè¿‡å°†æ¯ä¸ªè¦ç´ ç¼©æ”¾åˆ°ç»™å®šèŒƒå›´æ¥å˜æ¢è¦ç´ :**

```
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 4 samples/observations and 2 variables/features
data = np.array([[4, 6], [11, 34], [10, 17], [1, 5]])

# create scaler method
scaler = MinMaxScaler(feature_range=(0,1))

# fit and transform the data
scaled_data = scaler.fit_transform(data)

print(scaled_data)

# [[0.3        0.03448276]
#  [1\.         1\.        ] 
#  [0.9        0.4137931 ] 
#  [0\.         0\.        ]]
```

**å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬çš„æ•°æ®å·²ç»è¢«è½¬æ¢ï¼ŒèŒƒå›´åœ¨ **0** å’Œ **1** ä¹‹é—´ã€‚**

# **æ ‡å‡†åŒ–**

**æ ‡å‡†åŒ–ç¡®ä¿æ¯ä¸ªç‰¹å¾éƒ½æœ‰ä¸€ä¸ªå¹³å‡å€¼ **0** å’Œä¸€ä¸ªæ ‡å‡†åå·® **1** ï¼Œä½¿æ‰€æœ‰ç‰¹å¾è¾¾åˆ°ç›¸åŒçš„é‡çº§ã€‚**

**å¦‚æœç‰¹å¾çš„æ ‡å‡†åå·®æ˜¯ ***ä¸åŒ*** ï¼Œå®ƒä»¬çš„èŒƒå›´ä¹Ÿä¼šä¸åŒã€‚**

**![](img/d5c5d0d3a3d45abe3a93754a1769edd5.png)**

**x = observation, Î¼ = mean , Ïƒ = standard deviation**

**æ‚¨å¯ä»¥ä½¿ç”¨ Scikit-learn ä¸­çš„[**standard scaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)æ–¹æ³•ï¼Œé€šè¿‡ç§»é™¤å¹³å‡å€¼å¹¶ç¼©æ”¾è‡³æ ‡å‡†åå·® **1** æ¥æ ‡å‡†åŒ–ç‰¹å¾:**

```
from sklearn.preprocessing import StandardScaler
import numpy as np

# 4 samples/observations and 2 variables/features
data = np.array([[4, 1], [11, 1], [10, 4], [1, 11]])

# create scaler method 
scaler = StandardScaler()

# fit and transform the data
scaled_data = scaler.fit_transform(data)

print(scaled_data)

# [[-0.60192927 -0.79558708]
#  [ 1.08347268 -0.79558708] 
#  [ 0.84270097 -0.06119901] 
#  [-1.32424438  1.65237317]]
```

**è®©æˆ‘ä»¬éªŒè¯æ¯ä¸ªç‰¹å¾(åˆ—)çš„å¹³å‡å€¼æ˜¯ **0** :**

```
print(scaled_data.mean(axis=0))
```

**`[0\. 0.]`**

**å¹¶ä¸”æ¯ä¸ªç‰¹å¾(åˆ—)çš„æ ‡å‡†åå·®æ˜¯ **1** :**

```
print(scaled_data.std(axis=0))
```

**`[1\. 1.]`**

# **å¦‚ä½•å¤„ç†åˆ†ç±»ç‰¹å¾**

**åˆ†ç±»ç‰¹å¾è¡¨ç¤ºå¯ä»¥åˆ†ç»„çš„æ•°æ®ç±»å‹ã€‚æ¯”å¦‚æ€§åˆ«ï¼Œå—æ•™è‚²ç¨‹åº¦ã€‚**

**ä»»ä½•éæ•°å­—å€¼éƒ½éœ€è¦*è½¬æ¢*ä¸ºæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼Œä»¥ä¾¿åœ¨å¤§å¤šæ•°æœºå™¨å­¦ä¹ åº“ä¸­ä½¿ç”¨ã€‚**

**å¤„ç†åˆ†ç±»ç‰¹å¾çš„å¸¸ç”¨æ–¹æ³•æœ‰:**

# **æ ‡ç­¾ç¼–ç **

**æ ‡ç­¾ç¼–ç åªæ˜¯å°†ä¸€åˆ—ä¸­çš„æ¯ä¸ªåˆ†ç±»å€¼è½¬æ¢æˆä¸€ä¸ªæ•°å­—ã€‚**

**å»ºè®®ä½¿ç”¨æ ‡ç­¾ç¼–ç å°†å…¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶å˜é‡ã€‚**

**åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Scikit-learn ä¸­çš„ [**LableEncoder**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) å°†åˆ†ç±»å€¼è½¬æ¢ä¸ºäºŒè¿›åˆ¶:**

```
# import packages
import numpy as np 
import pandas as pd 
from sklearn.preprocessing import LabelEncoder

# intialise data of lists.
data = {'Gender':['male', 'female', 'female', 'male','male'],
        'Country':['Tanzania','Kenya', 'Tanzania', 'Tanzania','Kenya']}

# Create DataFrame
data = pd.DataFrame(data)

# create label encoder object
le = LabelEncoder()

data['Gender']= le.fit_transform(data['Gender'])
data['Country']= le.fit_transform(data['Country'])

print(data)
```

**![](img/20f86595cd15c9ee2cc460f24ea1c06f.png)**

**Transformed Data**

# **ä¸€æ¬¡çƒ­ç¼–ç **

**åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¡¨ç¤ºåˆ†ç±»å˜é‡æœ€å¸¸è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€é”®ç¼–ç ï¼Œæˆ– N é€‰ 1 ç¼–ç æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºå“‘å˜é‡ã€‚**

**è™šæ‹Ÿå˜é‡èƒŒåçš„æ€æƒ³æ˜¯ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå€¼ä¸º 0 å’Œ 1 çš„æ–°ç‰¹å¾æ›¿æ¢åˆ†ç±»å˜é‡ã€‚**

**![](img/08593087dcc9962221e45ab4df772f51.png)**

**åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Scikit-learn åº“ä¸­çš„ç¼–ç å™¨ã€‚ [**LabelEncoder**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) å°†å¸®åŠ©æˆ‘ä»¬ä»æ•°æ®ä¸­åˆ›å»ºæ ‡ç­¾çš„æ•´æ•°ç¼–ç ï¼Œè€Œ [**OneHotEncoder**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) å°†åˆ›å»ºæ•´æ•°ç¼–ç å€¼çš„ä¸€æ¬¡æ€§ç¼–ç ã€‚**

```
# import packages 
import numpy as np 
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# define example
data = np.array(['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot'])

# integer encode
label_encoder = LabelEncoder()

#fit and transform the data
integer_encoded = label_encoder.fit_transform(data)
print(integer_encoded)

# one-hot encode
onehot_encoder = OneHotEncoder(sparse=False)

#reshape the data
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)

#fit and transform the data
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

print(onehot_encoded)
```

**è¿™æ˜¯ **LabelEncoder** æ–¹æ³•å¯¹ **integer_encoded** çš„è¾“å‡º:**

**`[0 0 2 0 1 1 2 0 2 1]`**

**è¿™æ˜¯**onehotencoded**é€šè¿‡ **OneHotEncoder** æ–¹æ³•çš„è¾“å‡º:**

```
[[1\. 0\. 0.] 
 [1\. 0\. 0.] 
 [0\. 0\. 1.] 
 [1\. 0\. 0.] 
 [0\. 1\. 0.] 
 [0\. 1\. 0.] 
 [0\. 0\. 1.] 
 [1\. 0\. 0.] 
 [0\. 0\. 1.] 
 [0\. 1\. 0.]]
```

# **ä»€ä¹ˆæ˜¯ç‰¹å¾é€‰æ‹©ï¼Ÿ**

**è¦ç´ é€‰æ‹©æ˜¯è‡ªåŠ¨æˆ–æ‰‹åŠ¨é€‰æ‹©å¯¹é¢„æµ‹å˜é‡æˆ–è¾“å‡ºè´¡çŒ®æœ€å¤§çš„è¦ç´ çš„è¿‡ç¨‹ã€‚**

**![](img/54a7511cf45956839503301e3faceed5.png)**

**æ•°æ®ä¸­å­˜åœ¨ä¸ç›¸å…³çš„ç‰¹å¾ä¼šé™ä½æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚**

**ä½¿ç”¨ç‰¹å¾é€‰æ‹©çš„ä¸»è¦åŸå› æ˜¯:**

*   **å®ƒä½¿æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿæ›´å¿«åœ°è®­ç»ƒã€‚**
*   **å®ƒé™ä½äº†æ¨¡å‹çš„å¤æ‚æ€§ï¼Œä½¿å…¶æ›´å®¹æ˜“è§£é‡Šã€‚**
*   **å¦‚æœé€‰æ‹©äº†æ­£ç¡®çš„å­é›†ï¼Œå°±å¯ä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚**
*   **å®ƒå‡å°‘äº†è¿‡åº¦æ‹Ÿåˆã€‚**

> ***â€œæˆ‘é€šè¿‡é€‰æ‹©æ‰€æœ‰ç‰¹å¾å‡†å¤‡äº†ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘å¾—åˆ°äº†å¤§çº¦****65%****çš„å‡†ç¡®åº¦ï¼Œè¿™å¯¹äºé¢„æµ‹æ¨¡å‹æ¥è¯´ä¸æ˜¯å¾ˆå¥½ï¼Œåœ¨åšäº†ä¸€äº›ç‰¹å¾é€‰æ‹©å’Œç‰¹å¾å·¥ç¨‹åï¼Œæ²¡æœ‰å¯¹æˆ‘çš„æ¨¡å‹ä»£ç åšä»»ä½•é€»è¾‘æ›´æ”¹ï¼Œæˆ‘çš„å‡†ç¡®åº¦è·ƒå‡åˆ°äº†***81%*ï¼Œè¿™éå¸¸ä»¤äººå°è±¡æ·±åˆ»â€â€”â€”ä½œè€… Raheel Shaikh*****

***ç‰¹å¾é€‰æ‹©çš„å¸¸ç”¨æ–¹æ³•æœ‰:***

# ***å•å˜é‡é€‰æ‹©***

***ç»Ÿè®¡æµ‹è¯•æœ‰åŠ©äºé€‰æ‹©ä¸æ•°æ®é›†ä¸­çš„ç›®æ ‡è¦ç´ å…³ç³»æœ€å¯†åˆ‡çš„ç‹¬ç«‹è¦ç´ ã€‚æ¯”å¦‚å¡æ–¹æ£€éªŒã€‚***

***Scikit-learn åº“æä¾›äº† [**SelectKBest**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) ç±»ï¼Œè¯¥ç±»å¯ç”¨äºä¸€å¥—ä¸åŒçš„ç»Ÿè®¡æµ‹è¯•ï¼Œä»¥é€‰æ‹©ç‰¹å®šæ•°é‡çš„ç‰¹æ€§ã€‚***

***åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¸¦æœ‰å¡æ°æ£€éªŒçš„ **SelectKBest** ç±»æ¥å¯»æ‰¾è™¹è†œæ•°æ®é›†çš„æœ€ä½³ç‰¹å¾:***

```
*# Load packages
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# Load iris data
iris_dataset = load_iris()

# Create features and target
X = iris_dataset.data
y = iris_dataset.target

# Convert to categorical data by converting data to integers
X = X.astype(int)

# Two features with highest chi-squared statistics are selected
chi2_features = SelectKBest(chi2, k = 2)
X_kbest_features = chi2_features.fit_transform(X, y)

# Reduced features
print('Original feature number:', X.shape[1])
print('Reduced feature number:', X_kbest_features.shape[1])*
```

***åŸå§‹ç‰¹å¾å·:4
å‡å°‘çš„ç‰¹å¾å·:2***

***å¦‚æ‚¨æ‰€è§ï¼Œå¡æ–¹æ£€éªŒå¸®åŠ©æˆ‘ä»¬ä»æœ€åˆçš„ 4 ä¸ªç‰¹å¾ä¸­é€‰æ‹©å‡ºä¸ç›®æ ‡ç‰¹å¾å…³ç³»æœ€å¯†åˆ‡çš„ä¸¤ä¸ªé‡è¦çš„ç‹¬ç«‹ç‰¹å¾ã€‚***

***ä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºå¡æ–¹æ£€éªŒçš„ä¿¡æ¯:[â€œæœºå™¨å­¦ä¹ å¡æ–¹æ£€éªŒçš„æ¸©å’Œä»‹ç»](https://machinelearningmastery.com/chi-squared-test-for-machine-learning/)â€ã€‚***

# ***ç‰¹å¾é‡è¦æ€§***

***è¦ç´ é‡è¦æ€§ä¸ºæ•°æ®çš„æ¯ä¸ªè¦ç´ æä¾›äº†ä¸€ä¸ªåˆ†æ•°ã€‚åˆ†æ•°è¶Šé«˜ï¼Œ**è¶Šé‡è¦**æˆ–**ç›¸å…³**è¯¥åŠŸèƒ½å¯¹ä½ çš„ç›®æ ‡åŠŸèƒ½è¶Šé‡è¦ã€‚***

***ç‰¹å¾é‡è¦æ€§æ˜¯ä¸€ä¸ªå†…ç½®ç±»ï¼Œå¸¦æœ‰åŸºäºæ ‘çš„åˆ†ç±»å™¨ï¼Œä¾‹å¦‚:***

*   ***éšæœºæ£®æ—åˆ†ç±»å™¨***
*   ***é¢å¤–æ ‘åˆ†ç±»å™¨***

***åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠé¢å¤–çš„æ ‘åˆ†ç±»å™¨è®­ç»ƒåˆ° iris æ•°æ®é›†ä¸­ï¼Œå¹¶ä½¿ç”¨å†…ç½®çš„ç±»**ã€‚feature_importances_** è®¡ç®—æ¯ä¸ªç‰¹æ€§çš„é‡è¦æ€§:***

```
*# Load libraries
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.ensemble import ExtraTreesClassifier

# Load iris data
iris_dataset = load_iris()

# Create features and target
X = iris_dataset.data
y = iris_dataset.target

# Convert to categorical data by converting data to integers
X = X.astype(int)

 # Building the model
extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,
                                        criterion ='entropy', max_features = 2)

# Training the model
extra_tree_forest.fit(X, y)

# Computing the importance of each feature
feature_importance = extra_tree_forest.feature_importances_

# Normalizing the individual importances
feature_importance_normalized = np.std([tree.feature_importances_ for tree in 
                                        extra_tree_forest.estimators_],
                                        axis = 0)

# Plotting a Bar Graph to compare the models
plt.bar(iris_dataset.feature_names, feature_importance_normalized)
plt.xlabel('Feature Labels')
plt.ylabel('Feature Importances')
plt.title('Comparison of different Feature Importances')
plt.show()*
```

***![](img/45f50c9816d73bafe2dd4cce9bcda246.png)***

***Important features***

***ä¸Šå›¾æ˜¾ç¤ºæœ€é‡è¦çš„ç‰¹å¾æ˜¯*å’Œ ***èŠ±ç“£å®½åº¦(cm)*** ï¼Œæœ€ä¸é‡è¦çš„ç‰¹å¾æ˜¯ ***è¼ç‰‡å®½åº¦(cms)*** ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨æœ€é‡è¦çš„åŠŸèƒ½æ¥è®­ç»ƒæ‚¨çš„æ¨¡å‹å¹¶è·å¾—æœ€ä½³æ€§èƒ½ã€‚****

# ****ç›¸å…³çŸ©é˜µçƒ­å›¾****

****ç›¸å…³æ€§æ˜¾ç¤ºäº†ç‰¹å¾ä¹‹é—´æˆ–ä¸ç›®æ ‡ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚****

****ç›¸å…³æ€§å¯ä»¥æ˜¯æ­£çš„(å¢åŠ ä¸€ä¸ªç‰¹å¾å€¼ä¼šå¢åŠ ç›®æ ‡å˜é‡çš„å€¼)ï¼Œä¹Ÿå¯ä»¥æ˜¯è´Ÿçš„(å¢åŠ ä¸€ä¸ªç‰¹å¾å€¼ä¼šå‡å°‘ç›®æ ‡å˜é‡çš„å€¼)ã€‚****

****åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Scikit-learn åº“ä¸­çš„ Boston house prices æ•°æ®é›†å’Œ pandas ä¸­çš„[**corr()**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)**æ–¹æ³•æ¥æŸ¥æ‰¾ dataframe ä¸­æ‰€æœ‰è¦ç´ çš„æˆå¯¹ç›¸å…³æ€§:******

```
****# Load libraries
from sklearn.datasets import load_boston
import matplotlib.pyplot as plt
import seaborn as sns

# load boston data
boston_dataset = load_boston()

# create a daframe for boston data
boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)

# Convert to categorical data by converting data to integers
#X = X.astype(int)

#ploting the heatmap for correlation
ax = sns.heatmap(boston.corr().round(2), annot=True)****
```

******![](img/8a87796a9f0f5e5aad390d9eeecdfe42.png)******

******ç›¸å…³ç³»æ•°èŒƒå›´ä»-1 åˆ° 1ã€‚å¦‚æœè¯¥å€¼æ¥è¿‘ 1ï¼Œåˆ™æ„å‘³ç€è¿™ä¸¤ä¸ªç‰¹å¾ä¹‹é—´æœ‰å¾ˆå¼ºçš„æ­£ç›¸å…³æ€§ã€‚å½“æ¥è¿‘-1 æ—¶ï¼Œç‰¹å¾å…·æœ‰å¾ˆå¼ºçš„è´Ÿç›¸å…³æ€§ã€‚******

******åœ¨ä¸Šå›¾ä¸­ï¼Œå¯ä»¥çœ‹åˆ°**ç¨**å’Œ **RAD** ç‰¹å¾å…·æœ‰ s *trong æ­£ç›¸å…³*è€Œ **DIS** å’Œ **NOX** ç‰¹å¾å…·æœ‰*å¼ºè´Ÿç›¸å…³ã€‚*******

******å¦‚æœæ‚¨å‘ç°æ•°æ®é›†ä¸­æœ‰ä¸€äº›ç›¸äº’å…³è”çš„è¦ç´ ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¼ è¾¾äº†ç›¸åŒçš„ä¿¡æ¯ã€‚å»ºè®®å»æ‰å…¶ä¸­ä¸€ä¸ªã€‚******

******ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äºè¿™æ–¹é¢çš„å†…å®¹:[åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œä¸ºä»€ä¹ˆæœ‰å…³è”çš„ç‰¹å¾æ˜¯ä¸å¥½çš„ï¼Ÿ](https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features)******

# ******ç»“è®º******

******æˆ‘åœ¨æœ¬æ–‡ä¸­è§£é‡Šçš„æ–¹æ³•å°†å¸®åŠ©æ‚¨å‡†å¤‡å¤§éƒ¨åˆ†çš„**ç»“æ„åŒ–æ•°æ®é›†**ã€‚ä½†æ˜¯å¦‚æœæ‚¨æ­£åœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®é›†ï¼Œæ¯”å¦‚å›¾åƒã€æ–‡æœ¬å’ŒéŸ³é¢‘ï¼Œæ‚¨å°†ä¸å¾—ä¸å­¦ä¹ æœ¬æ–‡ä¸­æ²¡æœ‰è§£é‡Šçš„ä¸åŒæ–¹æ³•ã€‚******

******ä»¥ä¸‹æ–‡ç« å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¸ºæœºå™¨å­¦ä¹ é¡¹ç›®å‡†å¤‡å›¾åƒæˆ–æ–‡æœ¬æ•°æ®é›†:******

*   ******[ä¸º CNN å‡†å¤‡å’Œæ‰©å……å›¾åƒæ•°æ®çš„æœ€ä½³å®è·µâ€”â€”æ°æ£®Â·å¸ƒæœ—åˆ©](https://machinelearningmastery.com/best-practices-for-preparing-and-augmenting-image-data-for-convolutional-neural-networks/)******
*   ******[å›¾åƒé¢„å¤„ç†â€”â€”å¡åŠªç›ç‹å­](https://towardsdatascience.com/image-pre-processing-c1aec0be3edf)******
*   ******[NLP æ–‡æœ¬é¢„å¤„ç†:å®ç”¨æŒ‡å—å’Œæ¨¡æ¿â€”â€”ç¿å®¶è±ª](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79)******
*   ******[å¦‚ä½•ä½¿ç”¨ Texthero ä¸ºæ‚¨çš„ NLP é¡¹ç›®å‡†å¤‡åŸºäºæ–‡æœ¬çš„æ•°æ®é›†â€”â€”Davis David](https://www.freecodecamp.org/news/how-to-work-and-understand-text-based-dataset-with-texthero/)******

********æ­å–œ**ğŸ‘ğŸ‘**ï¼Œ**ä½ å·²ç»ç†¬åˆ°è¿™ç¯‡æ–‡ç« çš„ç»“å°¾äº†ï¼æˆ‘å¸Œæœ›ä½ å­¦åˆ°äº†ä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œå¯¹ä½ çš„ä¸‹ä¸€ä¸ªæœºå™¨å­¦ä¹ æˆ–æ•°æ®ç§‘å­¦é¡¹ç›®æœ‰æ‰€å¸®åŠ©ã€‚******

******å¦‚æœä½ å­¦åˆ°äº†æ–°çš„ä¸œè¥¿æˆ–è€…å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œè¯·åˆ†äº«ç»™å…¶ä»–äººçœ‹ã€‚åœ¨é‚£ä¹‹å‰ï¼Œä¸‹æœŸå¸–å­å†è§ï¼******

******ä½ ä¹Ÿå¯ä»¥åœ¨ Twitter ä¸Šæ‰¾åˆ°æˆ‘ [@Davis_McDavid](https://twitter.com/Davis_McDavid) ã€‚******

******æœ¬æ–‡é¦–å‘äº [Freecodecampã€‚](https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/)******