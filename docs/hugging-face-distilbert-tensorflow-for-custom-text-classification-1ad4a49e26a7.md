# ç”¨äºè‡ªå®šä¹‰æ–‡æœ¬åˆ†ç±»çš„æ‹¥æŠ±è„¸è’¸é¦å’Œå¼ é‡æµã€‚

> åŸæ–‡ï¼š<https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7?source=collection_archive---------0----------------------->

## é€æ­¥æŒ‡å—

## å¦‚ä½•é€šè¿‡ TensorFlow çš„æ‹¥æŠ±è„¸ API å¾®è°ƒæ–‡æœ¬äºŒè¿›åˆ¶åˆ†ç±»çš„ DistilBERTï¼Ÿ

![](img/78526cf363bb1a9d856e205070aa96c6.png)

Photo by [Jason Leung](https://unsplash.com/@ninjason?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/bookshelf?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

# ä»‹ç»ã€‚

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°ä¸€ä¸ªä½¿ç”¨[è¿ç§»å­¦ä¹ ](https://machinelearningmastery.com/transfer-learning-for-deep-learning/#:~:text=Transfer%20learning%20is%20a%20machine,model%20on%20a%20second%20task.)æŠ€æœ¯çš„äºŒè¿›åˆ¶æ–‡æœ¬åˆ†ç±»å®ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ *DistilBertï¼Œä¸€ä¸ªæ¥è‡ªæ‹¥æŠ±è„¸*å˜å½¢é‡‘åˆš*åº“çš„*é¢„è®­ç»ƒæ¨¡å‹åŠå…¶ç”¨äº *Tensorflow* çš„ APIã€‚

# ä¸ºä»€ä¹ˆè’¸é¦ã€‚

> [æ›´å°ã€æ›´å¿«ã€æ›´ä¾¿å®œã€æ›´è½»:æ¨å‡ºè’¸é¦ç‰ˆ DistilBERTã€‚](/huggingface/distilbert-8cf3380435b5)

Medium ä¸Šçš„*æ‹¥æŠ±è„¸*çš„è¯„è®ºæ–‡ç« æ ‡é¢˜ç»™å‡ºäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥åœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸­ä½¿ç”¨è¿™ä¸ªæ¨¡å‹çš„å®Œæ•´è§£é‡Šã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå°çš„æ•°æ®é›†ï¼Œè¿™ä¸ªæ¨¡å‹å¯èƒ½æ˜¯æˆ‘ä»¬å°è¯•çš„ä¸€ä¸ªä¸é”™çš„é¦–é€‰ã€‚å¦å¤–ï¼Œ[å¦ä¸€ç¯‡å…³äº Medium](https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8) çš„æ–‡ç« å»ºè®®ä½¿ç”¨ *DistilBERT* ä½œä¸ºå¿«é€ŸåŸºçº¿æ¨¡å‹ã€‚ *DistilBERT* å¯ä»¥åœ¨ *BERT* çš„è¡¨ç°ä¸Šè¾¾åˆ°ä¸€ä¸ªåˆç†çš„ä¸‹é™ï¼Œå…·æœ‰è®­ç»ƒæ›´å¿«çš„ä¼˜åŠ¿ã€‚è¿™ä¸ª API æˆ‘ä»¬åœ¨ä¸€ä¸ªå†™å¾—éå¸¸å¥½çš„ [*æŠ±æŠ±è„¸*æ–‡æ¡£](https://huggingface.co/transformers/master/model_doc/distilbert.html)å’Œ [*Tensorflow* åšå®¢](https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html)ã€‚ä½ ä¼šå‘ç°åº”ç”¨å®ƒæ˜¯å¤šä¹ˆç®€å•å’Œç›´è§‚ã€‚

> ä¸€ä¸ªæ›´å°çš„é€šç”¨è¯­è¨€è¡¨ç¤ºæ¨¡å‹ï¼Œç§°ä¸º DistilBERTï¼Œå®ƒå¯ä»¥åƒå®ƒçš„æ›´å¤§å¯¹åº”ç‰©ä¸€æ ·åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸Šæœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚

å¦‚æœä½ è¿˜åœ¨çŠ¹è±«ä»*æŠ±è„¸*åº“ä¸­é€‰æ‹©å“ªä¸ªæ¨¡å‹ï¼Œä½ å¯ä»¥ç”¨ä»–ä»¬çš„[è¿‡æ»¤å™¨](https://huggingface.co/models?filter=en,tf&pipeline_tag=text-classification)æŒ‰ä»»åŠ¡ã€åº“ã€è¯­è¨€ç­‰é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ã€‚ *DistilBERT* æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡*åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ª(*æ˜¯[distil Bert-base-uncased](https://huggingface.co/distilbert-base-uncased)çš„ä¸€ä¸ªå¾®è°ƒæ£€æŸ¥ç‚¹ï¼Œåœ¨ SST-2 ä¸Šå¾®è°ƒ)ã€‚æ‰€ä»¥æˆ‘ä»¬é€‰æ‹©äº†å®ƒâ€”â€”å¤ªå¥½äº†ï¼

# å¯¹æ•°æ®çš„å›é¡¾ã€‚

ä»£ç ç¤ºä¾‹çš„æ•°æ®æ¥è‡ªæˆ‘ä¹‹å‰çš„é¡¹ç›®ã€‚æˆ‘ä»ä¸€ä¸ªèœè°±ç½‘ç«™ä¸Šæ”¶é›†çš„ï¼Œæ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†(æ‹†åˆ†æ¯”ä¾‹ 0.2)ã€‚æ•°æ®é›†åŒ…å«ä¸€åˆ—æºæ–‡æœ¬(ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ•°æ®é›†[æˆ–æŸ¥çœ‹è¿™ä¸ª](/codestory/nlp-text-pre-processing-and-feature-engineering-python-69338fa0372e)[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1YOZ60sdOjSbIiB3IiJVUYprQhJDEVxXB?usp=sharing))ã€‚è¿˜æœ‰ä¸€ä¸ªå¸¦æ ‡ç­¾çš„åˆ—*ã€‚***ä¸šåŠ¡ç›®æ ‡**æ˜¯ç¡®å®šæ¯æ®µçš„æ ‡ç­¾æ˜¯*â€œé…æ–™â€*è¿˜æ˜¯*â€œé…æ–¹è¯´æ˜â€ã€‚*

è®©æˆ‘ä»¬å®‰è£…ã€å¯¼å…¥åº“ï¼Œå¹¶ä¸ºæ¨¡å‹çš„è¶…å‚æ•°å®šä¹‰å¸¸æ•°:

```
!pip install transformersimport pandas as pd
import tensorflow as tf
import transformers
from transformers import DistilBertTokenizer
from transformers import TFDistilBertForSequenceClassificationpd.set_option('display.max_colwidth', None)
MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'
BATCH_SIZE = 16
N_EPOCHS = 3
```

ç›®å‰ï¼Œæˆ‘ä»¬åªå¯¹â€œæ®µè½â€å’Œâ€œæ ‡ç­¾â€åˆ—æ„Ÿå…´è¶£ã€‚çœ‹ä¸‹å›¾(å›¾ 1):â€œæ®µè½â€ä¸­çš„æ–‡æœ¬æ˜¯ä¸€ä¸ªæºæ–‡æœ¬ï¼Œè€Œä¸”æ˜¯ç”¨å­—èŠ‚è¡¨ç¤ºçš„ã€‚åœ¨ X_train é›†åˆä¸­ï¼Œæˆ‘ä»¬æœ‰ 3898 è¡Œï¼ŒX_test é›†åˆæœ‰ 973 è¡Œã€‚åœ¨è¿™äº›é›†åˆä¸­æ²¡æœ‰ NaNs æˆ–ç©ºå­—ç¬¦ä¸²ã€‚

![](img/f089ab863523e25be6321871817014b9.png)

Pic.1 Load Train and Test data sets, a sample from X_train, shape check.

å¦‚æœæ®µè½æ˜¯â€œé…æ–¹æˆåˆ†â€ï¼Œç›®æ ‡å˜é‡æ˜¯â€œ1â€ï¼Œå¦‚æœæ˜¯â€œè¯´æ˜â€ï¼Œç›®æ ‡å˜é‡æ˜¯â€œ0â€ã€‚æ ‡ç­¾çš„æ¯”ä¾‹å¤§çº¦æ˜¯ 20% 1 å’Œ 80% 0ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿›å…¥ä¸‹ä¸€æ­¥ã€‚

# å‡†å¤‡æ•°æ®ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚

åœ¨æ–‡æœ¬æˆä¸ºæ¨¡å‹è¾“å…¥ä¹‹å‰ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥å¯¹å®ƒè¿›è¡Œæ ‡è®°åŒ–ã€‚[*distilbertokenizer*](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer)æ¥å—â€œstrâ€(å•ä¸ªç¤ºä¾‹)ã€â€œList[str]â€(æ‰¹é‡æˆ–å•ä¸ªé¢„æ ‡è®°ç¤ºä¾‹)æˆ–â€œList[List[str]]â€(æ‰¹é‡é¢„æ ‡è®°ç¤ºä¾‹)ç±»å‹çš„æ–‡æœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†å­—èŠ‚è¡¨ç¤ºè½¬æ¢æˆå­—ç¬¦ä¸²ã€‚Lambda å‡½æ•°æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è§£å†³æ–¹æ¡ˆã€‚

```
X_train = X_train.apply(lambda x: str(x[0], 'utf-8'))
X_test = X_test.apply(lambda x:  str(x[0], 'utf-8'))
```

DistilBERT ä¸­æœ€å¤§æ”¯æŒçš„æ ‡è®°åŒ–å¥å­é•¿åº¦æ˜¯ 512 ä¸ªå•è¯ã€‚

```
*#define a tokenizer object* tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)#tokenize the text
train_encodings = tokenizer(list(X_train.values),
                            truncation=True, 
                            padding=True)test_encodings = tokenizer(list(X_test.values),
                           truncation=True, 
                           padding=True)
```

æˆ‘ä»¬ä¼ é€’ç»™è®°å·èµ‹äºˆå™¨çš„å‚æ•° hold:æˆ‘ä»¬çš„ set in "list[str]"è¡¨ç¤ºï¼Œtruncation=Trueï¼Œpadding=Trueã€‚å¦‚æœæ ‡è®°åŒ–å¥å­é•¿åº¦å°äºæœ€å¤§æ¨¡å‹è¾“å…¥é•¿åº¦ï¼Œæ ‡è®°åŒ–å™¨å°†å…¶æˆªæ–­åˆ°æ ‡è®°åŒ–å¥å­æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ ‡è®°åŒ–çš„å¥å­é•¿åº¦å°äºæœ€å¤§æ ‡è®°åŒ–çš„å¥å­é•¿åº¦ï¼Œæ ‡è®°åŒ–å™¨ç”¨é›¶å¡«å……ï¼Œç›´åˆ°æœ€å¤§æ ‡è®°åŒ–çš„å¥å­é•¿åº¦ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ç»“æœç¤ºä¾‹:

![](img/a1e04486ccd83c615591d495e5bb2de5.png)

Pic2\. An example of tokenized sentece by DistilBertTokenizer.

DistilBertTokenizer å¼•ç”¨è¶…ç±»`[**BertTokenizer**](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertTokenizer)**.**`ï¼Œå®ƒè¿”å›ç»™æˆ‘ä»¬ä¸€ç»„è¾“å…¥[ç´¢å¼•](https://huggingface.co/transformers/glossary.html#input-ids)å’Œ[æ³¨æ„å±è”½](https://huggingface.co/transformers/glossary.html#attention-mask)ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬åªéœ€è¦å°†æ ‡ç­¾å’Œç¼–ç è½¬æ¢æˆ Tensorflow æ•°æ®é›†å¯¹è±¡:

```
train_dataset = 
tf.data.Dataset.from_tensor_slices((dict(train_encodings),
                                    list(y_train.values)))
test_dataset = 
tf.data.Dataset.from_tensor_slices((dict(test_encodings),
                                    list(y_test.values)))
```

# ä½¿ç”¨ native TensorFlow è¿›è¡Œå¾®è°ƒã€‚

åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ TFDistilBertForSequenceClassificationï¼Œå¹¶å°†æ¨¡å‹çš„åç§°ä½œä¸ºå‚æ•°ã€‚è®¾ç½®å­¦ä¹ ç‡å¹¶å®šä¹‰æŸå¤±å‡½æ•°ã€‚ç¼–è¯‘æ¨¡å‹å¹¶è¿è¡Œ model.fit()æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚

```
model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)*#chose the optimizer* optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)*#define the loss function* losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)#build the model
model.compile(optimizer=optimizerr,
              loss=losss,
              metrics=['accuracy'])# train the model 
model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),
          epochs=N_EPOCHS,
          batch_size=BATCH_SIZE)
```

ä¸‹é¢ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹æœ‰å¤šç²¾ç¡®ã€‚åœ¨ç¬¬äºŒä¸ªæ—¶æœŸï¼Œæˆ‘ä»¬å·²ç»è·å¾—äº† 100%çš„å‡†ç¡®åº¦:

```
>>> Epoch 1/3
>>> 244/244 [==============================] - 131s 374ms/step - 
>>> loss: 0.1468 - accuracy: 0.9568
>>> Epoch 2/3
>>> 244/244 [==============================] - 95s 388ms/step - 
>>> loss: 3.1370e-04 - accuracy: 1.0000
>>> Epoch 3/3
>>> 244/244 [==============================] - 97s 396ms/step -
>>> loss: 5.7763e-05 - accuracy: 1.0000
```

# æ¨¡å‹è¯„ä¼°ã€‚

Tensorflow çš„æ‹¥æŠ±è„¸ API å¯¹ä»»ä½•æ•°æ®ç§‘å­¦å®¶æ–¹æ³•éƒ½å…·æœ‰ç›´è§‚æ€§ã€‚è®©æˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¹¶åœ¨æ–°æ•°æ®å‡ºç°ä¹‹å‰è¿›è¡Œè¯„ä¼°:

```
*# model evaluation on the test set* model.evaluate(test_dataset.shuffle(len(X_test)).batch(BATCH_SIZE), 
               return_dict=True, 
               batch_size=BATCH_SIZE) >>> 61/61 [==============================] - 10s 147ms/step - 
>>> loss: 1.7124e-05 - accuracy: 1.0000
>>> {'accuracy': 1.0, 'loss': 1.7123966244980693e-05}
```

æˆ‘ä»¬å¾—åˆ°äº†ç›¸å½“å¥½çš„ç»“æœï¼ç°åœ¨ï¼Œå¯¹äºå…¶ä»–æ–‡æœ¬æ®µè½çš„æ¨¡å‹ä¼°è®¡ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æŸ¥çœ‹æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡(ä»¥æŸ¥çœ‹æˆ‘ä»¬çš„æ¨¡å‹åœ¨é¢„æµ‹ä¸­æœ‰å¤šç¡®å®š):

```
def predict_proba(text_list, model, tokenizer):   *#tokenize the text* encodings = tokenizer(text_list, 
                          max_length=MAX_LEN, 
                          truncation=True, 
                          padding=True)
 *#transform to tf.Dataset*    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings))) #predict
    preds = model.predict(dataset.batch(1)).logits  

    #transform to array with probabilities
    res = tf.nn.softmax(preds, axis=1).numpy()      

    return res
```

æˆ‘ä»¬è¿™é‡Œå–ä¸€ä¸ª. txt æ–‡ä»¶ã€‚è¿™ä¸ªæ–‡ä»¶åŒ…å« 10 ä¸ªé£Ÿè°±é¡µé¢çš„ 10 ä¸ª URLã€‚æˆ‘ä»¬çš„æ¨¡å‹è¿˜æ²¡æœ‰çœ‹åˆ°æ¥è‡ªå®ƒä»¬çš„æ–‡æœ¬æ•°æ®ã€‚å‡è®¾æ‚¨ä»ç¬¬ä¸€ä¸ª [URL](https://www.loveandlemons.com/green-bean-salad-recipe/) è·å–æ•°æ®ã€‚æ‚¨è¾“å…¥åˆ°é¢„æµ‹æ¨¡å‹ä¸­çš„å­—ç¬¦ä¸²åˆ—è¡¨å°†ç±»ä¼¼äºä¸‹é¢çš„å•å…ƒæ ¼ã€‚(*ç¬¬ä¸€ä¸²æ˜¯é…æ–™ï¼Œåé¢ä¸‰ä¸²æ˜¯è¯´æ˜):

```
strings_list =["""
                  1 pound green beans, trimmed
                  Â½ head radicchio, sliced into strips
                  Scant Â¼ cup thinly sliced red onion
                  Honey Mustard Dressing, for drizzling
                  2 ounces goat cheese
                  2 tablespoons chopped walnuts
                  2 tablespoons sliced almonds
                  Â¼ cup tarragon
                  Flaky sea salt""", """Bring a large pot of salted water to a boil and                  set a bowl of ice water nearby. Drop the green beans into the boiling water and blanch for 2 minutes. Remove the beans and immediately immerse in the ice water long enough to cool completely, about 15 seconds. Drain and place on paper towels to dry. """, """Transfer the beans to a bowl and toss with the radicchio, onion, and a few spoonfuls of the dressing.""", """Arrange on a platter and top with small dollops of goat cheese, the walnuts, almonds, and tarragon. Drizzle with more dressing, season to taste with flaky salt, and serve."""]
```

å½“æ‚¨ä¸ºæ–°æ•°æ®è°ƒç”¨ predict_proba()å‡½æ•°æ—¶ï¼Œç»“æœå°†æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º(4ï¼Œ2)çš„ NumPy æ•°ç»„ã€‚å››ä¸ª n æ•°ç»„(æ¯ä¸ªæ®µè½ä¸€ä¸ª)ï¼Œå…·æœ‰ä¸¤ä¸ªæ¦‚ç‡å€¼(å¯¹äºç±» 0 å’Œç±» 1):

```
predict_proba(string1, model, tokenizer)>>> array([
>>> [1.63417135e-05, 9.99983668e-01],
>>> [9.99986053e-01, 1.39580325e-05],
>>> [9.99986053e-01, 1.39833473e-05],        
>>> [9.99988914e-01, 1.11078716e-05]], dtype=float32)
```

è¯¥æ¨¡å‹çš„é¢„æµ‹éå¸¸å‡†ç¡®ã€‚

# ç»“è®ºã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨äº†è§£äº†å¦‚ä½•åœ¨å¸¦æœ‰è‡ªå®šä¹‰å°æ–‡æœ¬æ•°æ®çš„äºŒè¿›åˆ¶åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯¹ DistilBert(æ¥è‡ª Hugging Face Transformers åº“çš„é¢„è®­ç»ƒæ¨¡å‹)åŠå…¶ tensor flow API è¿›è¡Œå¾®è°ƒã€‚

*æœ¬æ–‡çš„*[*Google collab*](https://colab.research.google.com/drive/14PtqRbrGa70M5NJnLfSllyJYyvl1nYnK?usp=sharing)*ç¬”è®°æœ¬ã€‚*

*ä¸€ä¸ª* [*æŠ±è„¸æ–‡æ¡£é¡µé¢*](https://huggingface.co/transformers/master/community.html#community-notebooks) *ï¼Œå…¶ä¸­é‡æ–°ç»„åˆäº†å„åœ°çš„èµ„æºğŸ¤—ç¤¾åŒºå¼€å‘çš„å˜å½¢é‡‘åˆš*ã€‚