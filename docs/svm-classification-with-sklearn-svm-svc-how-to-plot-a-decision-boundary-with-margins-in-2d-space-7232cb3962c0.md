# ä½¿ç”¨ sklearn.svm.SVC è¿›è¡Œ SVM åˆ†ç±»:å¦‚ä½•åœ¨ 2D ç©ºé—´ä¸­ç»˜åˆ¶å¸¦æœ‰è¾¹è·çš„å†³ç­–è¾¹ç•Œ

> åŸæ–‡ï¼š<https://medium.com/geekculture/svm-classification-with-sklearn-svm-svc-how-to-plot-a-decision-boundary-with-margins-in-2d-space-7232cb3962c0?source=collection_archive---------2----------------------->

![](img/60b6dc54072049083fcaf77e1b2844cf.png)

Photo by [cottonbro](https://www.pexels.com/@cottonbro?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) from [Pexels](https://www.pexels.com/photo/healthy-wood-playing-sport-5740517/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)

å› æ­¤ï¼Œå¦‚æœä½ åŒå€¦äº†æŒ–æ˜å¤§é‡æ— ç”¨çš„é“¾æ¥ï¼Œå¸Œæœ›æ‰¾åˆ°å¦‚ä½•ç”¨æ”¯æŒå‘é‡æœºç®—æ³•ç»˜åˆ¶å†³ç­–è¾¹ç•Œå’Œè¾¹ç¼˜çº¿çš„è¯¦ç»†è§£é‡Šï¼Œé‚£ä¹ˆæ­å–œä½ â€”â€”ä½ ç»ˆäºæ‰¾åˆ°äº†ï¼è¿˜æœ‰æœ€åä¸€æ­¥â€”â€”é˜…è¯»æˆ‘å…³äºè¿™ä¸ªä¸»é¢˜çš„ç®€å•æŒ‡å—ã€‚å¼€å§‹å§ï¼

> çµæ„Ÿæ¥æº:[https://sci kit-learn . org/stable/auto _ examples/SVM/plot _ SVM _ margin . html](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html)

*æ‹œæ‰˜ï¼Œä¸è¦åªèµ°è¿‡ä¸Šé¢çš„é“¾æ¥ã€‚ä¹Ÿè®¸ï¼Œå®ƒä¼šå¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£è¿™ä¸ªè¯é¢˜ã€‚*

# 0.è®©æ¨¡ç‰¹å­¦ä¼šï¼

æˆ‘ç›¸ä¿¡ä½ å·²ç»ç†Ÿæ‚‰è¿™ä¸€æ­¥äº†ã€‚è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ï¼Œç„¶åæŒ‰è®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬è¿›è¡Œæ‹†åˆ†ï¼Œæœ€åç”¨`sklearn.svm.SVC(kernel='linear')`è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ç¡®åˆ‡åœ°ä½¿ç”¨äº†**çº¿æ€§**å†…æ ¸ç±»å‹(ä¸€äº›ä¾‹å­ä¸­çš„[é“¾æ¥](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html))ã€‚ä»”ç»†é˜…è¯»è¯„è®º:

```
**import numpy as np
from sklearn.svm import SVC**# Creating a random dataset of 2,000 samples and only 2 features
# (for 2â€“dimensional space). And yeah, it's a binary classification
# here (`y` contains two classes: 0 and 1).
**X, y = make_classification(n_samples=2000, n_features=2,
                           n_informative=2, n_redundant=0,
                           n_classes=2,
                           random_state=32)**# Splitting our dataset by train and test parts.
# `stratify` is here to make our splitting balanced
# in terms of classes.
**X_train, X_test, y_train, y_test = train_test_split(X, y,
                                   test_size=0.3, stratify=y,
                                   random_state=32)**# And here we train our model. IMPORTANT: we use kernel='linear'.
**svc_model = SVC(kernel='linear', random_state=32)
svc_model.fit(X_train, y_train)**
```

å¾ˆå¥½ï¼æ¨¡å‹å·²ç»è®­ç»ƒå¥½äº†ï¼Œç°åœ¨ä½ æƒ³ç»˜åˆ¶ä¸€ä¸ªå†³ç­–è¾¹ç•Œ(è¶…å¹³é¢)ï¼Œä½†æ˜¯ç­‰ç­‰â€¦â€¦ä½ ä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹ã€‚åˆ«æ‹…å¿ƒï¼Œæˆ‘ä¼šä¸€æ­¥æ­¥æŒ‡å¯¼ä½ ã€‚

# 1.æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ¥ç»˜åˆ¶å†³ç­–è¾¹ç•Œï¼Ÿ

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€šç”¨çš„ç­‰å¼:

![](img/54a8cc7647bb48d80b893b52768a38f1.png)

General equation of a decision boundary in a vector form

> **æ³¨:**ç”±äºæˆ‘ä»¬åœ¨äºŒç»´ç©ºé—´ä¸­æ“ä½œï¼Œå†³ç­–è¾¹ç•Œå°†ç”± 2D ç›´çº¿è¡¨ç¤ºï¼Œå› æ­¤`w`å’Œ`x`éƒ½ç”±ä¸¤ä¸ªå…ƒç´ ç»„æˆã€‚

è¯€çªå°±åœ¨è¿™é‡Œã€‚æˆ‘ä»¬è½¬æ¢çŸ¢é‡`w`å’Œ`x`çš„ç‚¹ç§¯ï¼Œç„¶åæ±‚è§£`x`çš„ç¬¬äºŒä¸ªå…ƒç´ çš„æ–¹ç¨‹:

![](img/4fe692a868b962961f4b795f3b0a6597.png)

From general equation to a 2D-line equation

ç»“æœï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ–œç‡æˆªè·å½¢å¼çš„ç»å…¸çº¿æ€§æ–¹ç¨‹ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒæè¿°äº†ä¸€æ¡ 2D ç›´çº¿ã€‚å¥½çš„ï¼Œæˆ‘ä»¬çŸ¥é“äº†ï¼Œä½†æ˜¯ä½ å¯èƒ½è¿˜æœ‰ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚:

1.  `w_1`å’Œ`w_2`åˆ°åº•å­˜å‚¨åœ¨æˆ‘ä»¬çš„`sklearn`æ¨¡å‹å¯¹è±¡çš„ä»€ä¹ˆåœ°æ–¹ï¼Ÿ
2.  è¿˜æœ‰ä»€ä¹ˆæ˜¯`x_1`å’Œ`x_2`ï¼Ÿ

çœ‹å›¾:

![](img/4d125356ddcff5d025162be5cd502977.png)

Revealing the parts of a 2D-line equation

`**w**`åŒ…å«åœ¨æˆ‘ä»¬çš„æ¨¡å‹(`**svc_model.coef_**`)çš„å±æ€§`**coef_**`ä¸­ï¼Œè¿™äº›æ˜¯æˆ‘ä»¬çš„å†³ç­–è¾¹ç•Œçš„æ³•å‘é‡çš„åæ ‡(è¯¥å‘é‡ä¸è¶…å¹³é¢æ­£äº¤)ã€‚

`**b**`æ˜¯æˆ‘ä»¬æ¨¡å‹(`**svc_model.intercept_**`)çš„ä¸€ä¸ªå±æ€§`**intercept_**`ã€‚

`**x_1**`æ˜¯ä¸€ä¸ªåæ ‡å¹³é¢çš„ **x ç‚¹**çš„æ•°ç»„ã€‚

`**x_2**`æ˜¯ä¸€ä¸ªåæ ‡å¹³é¢çš„ **y ç‚¹**çš„*ç»“æœ*æ•°ç»„ã€‚

> æ­£å¦‚ä½ æ‰€æ³¨æ„åˆ°çš„ï¼Œæˆ‘ä»¬åªæœ‰ 2 ä¸ªå…ƒç´ ç”¨äº`w`ï¼ŒåŒæ ·çš„å…ƒç´ ç”¨äº`x`ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ­£åœ¨æ¢ç´¢äºŒç»´ç©ºé—´ä¸­çš„ SVM åˆ†ç±»ï¼Œåªæ˜¯ä¸ºäº†è®©ç»˜åˆ¶ SVM å†³ç­–è¾¹ç•Œå’Œè¾¹é™…çš„æƒ³æ³•æ›´åŠ æ¸…æ™°ã€‚

# 2.æŠŠæ‰€æœ‰çš„ä¸œè¥¿æ”¾åœ¨ä¸€èµ·

åªè¦å¯¹ 2D çº¿å…¬å¼åšä¸€äº›æ›¿æ¢â€¦

![](img/c023a4788a175c9dbecfeaedafa33ec8.png)

â€¦åœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°:

```
**import matplotlib.pyplot as plt
import seaborn as sns****plt.figure(figsize=(10, 8))**# Plotting our two-features-space
**sns.scatterplot(x=X_train[:, 0], 
                y=X_train[:, 1], 
                hue=y_train, 
                s=8);**# Constructing a hyperplane using a formula.
**w =** **svc_model****.coef_[0]** # w consists of 2 elements **b =** **svc_model****.intercept_[0]** # b consists of 1 element **x_points = np.linspace(-1, 1)** # generating x-points from -1 to 1 **y_points = -(w[0] / w[1]) * x_points - b / w[1]** # getting corresponding y-points# Plotting a red hyperplane
**plt.plot(x_points, y_points, c='r');**
```

å®ƒçš„è¾“å‡ºæ˜¯:

![](img/b86fa26b1934a7f928bef583232fcf29.png)

Two features (thatâ€™s why we have exactly 2 axis), two classes (blue and yellow) and a red decision boundary (hyperplane) in a form of 2D-line

å¤ªå¥½äº†ï¼æˆ‘ä»¬ç°åœ¨å·²ç»ç»˜åˆ¶äº†å†³ç­–è¾¹ç•Œï¼å› æ­¤ï¼Œæ˜¯æ—¶å€™è½¬å‘ SVM åˆ©æ¶¦ç‡äº†ã€‚

# 4.ç»˜åˆ¶ SVM è¾¹è·

åœ¨è¿™é‡Œï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªå†³ç­–è¾¹ç•Œå’Œåˆ©æ¶¦çš„å›¾ç‰‡ã€‚ç°è‰²è™šçº¿æ˜¯â€œè¾¹ç•Œâ€,æ˜¾ç¤ºäº†å®ƒä»¬çš„åŸºç¡€ï¼Œä»¥åŠå¦‚ä½•è®¡ç®—å®ƒä»¬çš„å¤§å°ã€‚ä»”ç»†çœ‹å„æ–¹é¢:

![](img/852ea955d8f1b7f435113a9ab189c44a.png)

SVM classification illustrated. Decision boundary, margins, and support vectors.

å› æ­¤ï¼Œè™šçº¿åªæ˜¯æ²¿ç€å‘é‡`w`çš„æ–¹å‘å¹³ç§»äº†ç­‰äº`margin`çš„è·ç¦»çš„åˆ¤å®šè¾¹ç•Œçº¿ã€‚æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªç®—æ³•æ¥å®ç°:

```
**Step 1.** Find a normal vector to the decision boundary;**Step 2.** Calculate a unit vector of that normal vector -- let's call it `w_hat`;**Step 3.** Get a distance between the lines (margin);**Step 4.** Translate all points of the decision boundary to a new location by this formula:# for a line above
new_points_above = hyperplane_points **+** w_hat * margin# for a line below
new_points_below = hyperplane_points **-** w_hat * margin
```

**ç¬¬ä¸€æ­¥:**æˆ‘ä»¬å·²ç»çŸ¥é“ï¼Œ`w`æ˜¯æˆ‘ä»¬éœ€è¦çš„æ³•å‘é‡â€”â€”å®ƒæ€»æ˜¯æ­£äº¤äºè¶…å¹³é¢(é¡ºä¾¿è¯´ä¸€å¥ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆâ€”â€”[stack overflow link](https://datascience.stackexchange.com/questions/6054/in-svm-algorithm-why-vector-w-is-orthogonal-to-the-separating-hyperplane))ã€‚è€Œä¸”æˆ‘ä»¬è¿˜çŸ¥é“å®ƒåŒ…å«åœ¨æˆ‘ä»¬æ¨¡å‹çš„å±æ€§`**coef_**`(`**svc_model.coef_**`)ä¸­ã€‚

**ç¬¬äºŒæ­¥:**æˆ‘ä»¬æ¥è®¡ç®—å•ä½çŸ¢é‡:

![](img/6ba626daebfa60c96fbd1e457cf6e35c.png)

Unit-vector formula

```
# Calculating the unit vector of w
**w_hat =** **svc_model****.coef_[0] / (np.sqrt(np.sum(****svc_model****.coef_[0] ** 2)))**
```

**ç¬¬ä¸‰æ­¥:**ç°åœ¨æˆ‘ä»¬è®¡ç®—ä¿è¯é‡‘:

![](img/d3f00dc2dfb04cb61e32210949acdc81.png)

Margin formula

```
# Calculating margin
**margin = 1 / np.sqrt(np.sum(svc_model.coef_[0] ** 2))**
```

ç¬¬å››æ­¥:æœ€åï¼Œæˆ‘ä»¬è®¡ç®—æ–°çº¿æ¡çš„ç‚¹æ•°:

```
# Calculating margin lines
**new_points_up   =** **hyperplane_points** **+ w_hat * margin
new_points_down =** **hyperplane_points** **- w_hat * margin**
```

è¿˜æœ‰ä¸€ä»¶äº‹â€¦æ˜¯çš„ï¼Œè®©æˆ‘ä»¬æŠŠå®ƒä»¬æ”¾åœ¨ä¸€èµ·ï¼Œç„¶åç”»å‡ºç»“æœ:

```
**plt.figure(figsize=(10, 8))**# Plotting our two-features-space
**sns.scatterplot(x=X_train[:, 0], 
                y=X_train[:, 1], 
                hue=y_train, 
                s=8);**# Constructing a hyperplane using a formula.
**w =** **svc_model****.coef_[0]** # w consists of 2 elements **b =** **svc_model****.intercept_[0]** # b consists of 1 element **x_points = np.linspace(-1, 1)** # generating x-points from -1 to 1 **y_points = -(w[0] / w[1]) * x_points - b / w[1]** # getting corresponding y-points# Plotting a red hyperplane
**plt.plot(x_points, y_points, c='r');**# Encircle support vectors
**plt.scatter(****svc_model****.support_vectors_[:, 0],** **svc_model****.support_vectors_[:, 1], 
            s=50, 
            facecolors='none', 
            edgecolors='k', 
            alpha=.5);**# Step 2 (unit-vector):
**w_hat =** **svc_model****.coef_[0] / (np.sqrt(np.sum(****svc_model****.coef_[0] ** 2)))**# Step 3 (margin):
**margin = 1 / np.sqrt(np.sum(****svc_model****.coef_[0] ** 2))**# Step 4 (calculate points of the margin lines):
**decision_boundary_points = np.array(list(zip(x_points, y_points)))
points_of_line_above = decision_boundary_points + w_hat * margin
points_of_line_below = decision_boundary_points - w_hat * margin**# Plot margin lines# Blue margin line above **plt.plot(points_of_line_above[:, 0], 
         points_of_line_above[:, 1], 
         'b--', 
         linewidth=2)**# Green margin line below **plt.plot(points_of_line_below[:, 0], 
         points_of_line_below[:, 1], 
         'g--',
         linewidth=2)**
```

![](img/84a999eab84e2f0ddde3db031ead5a86.png)

Final result: decision boundary (red), support vector (encircled dots), and margin lines (dashed blue and green lines)

æˆ‘ä»¬å®Œäº†ã€‚å¸Œæœ›å¯¹ä½ æœ‰å¸®åŠ©ã€‚å¦‚æœæ˜¯ï¼ŒæŒ‰ä¸‹`thumbs up ğŸ‘`ã€‚

æ„Ÿè°¢æ‚¨çš„å®è´µæ—¶é—´ï¼ğŸ‰
**å…³æ³¨æˆ‘**æ›´å¤šæœ‰è¶£è¯é¢˜ğŸ˜‰ğŸ‘