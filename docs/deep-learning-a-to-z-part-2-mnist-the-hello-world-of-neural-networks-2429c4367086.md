# æ·±åº¦å­¦ä¹ â€”â€”â€œä½ å¥½ï¼Œä¸–ç•Œï¼â€

> åŸæ–‡ï¼š<https://medium.com/geekculture/deep-learning-a-to-z-part-2-mnist-the-hello-world-of-neural-networks-2429c4367086?source=collection_archive---------0----------------------->

![](img/03c4f5af2ff7afd1260c37402563464d.png)

è¿™æ˜¯æˆ‘ä¸Šä¸€ç¯‡æ–‡ç« çš„å»¶ç»­ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰è¯»è¿‡ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ å…ˆè¯»ä¸€ä¸‹ï¼Œä»¥è·å¾—æ·±åº¦å­¦ä¹ æ¦‚å¿µçš„é«˜å±‚æ¬¡æ¦‚è¿°ã€‚è¿™æ˜¯ç¬¬ä¸€éƒ¨åˆ†çš„é“¾æ¥([æ·±åº¦å­¦ä¹ â€”â€”åˆå­¦è€…æŒ‡å—](https://srinivas-kulkarni.medium.com/deep-learning-a-to-z-part-1-1d5bd4e9944c))

æˆ‘æœ¬å¯ä»¥æ·±å…¥åˆ°æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ç­‰æ›´å¤šçš„ç†è®ºå’Œæ•°å­¦éƒ¨åˆ†ã€‚ä½†æ˜¯æˆ‘è®¤ä¸ºï¼Œè¿™æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œè®©æˆ‘ä»¬åŠ¨æ‰‹å®ç°ç¬¬ä¸€ä¸ªäººå·¥ç¥ç»ç½‘ç»œ(ANN)æ¥äº†è§£å·¥å…·é›†ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ã€‚

# å®‰è£…è½¯ä»¶å’Œè®¾ç½®ç¯å¢ƒ

é¦–å…ˆï¼Œæ‚¨éœ€è¦åœ¨æ‚¨çš„è®¡ç®—æœºä¸Šå®‰è£… Anaconda å‘è¡Œç‰ˆã€‚å¦‚æœæ‚¨è¿˜æ²¡æœ‰ï¼Œè¯·ä» [Anaconda](https://www.anaconda.com/products/individual) ç½‘ç«™ä¸‹è½½ã€‚å®‰è£…å®Œæˆåï¼Œæ‚¨éœ€è¦è®¾ç½®ç¯å¢ƒã€‚conda ç¯å¢ƒé€šå¸¸æ˜¯ä¸€ç»„ç›¸äº’å…¼å®¹çš„ç›¸å…³è½¯ä»¶ã€‚å½“æ‚¨åˆ›å»ºä¸€ä¸ªç¯å¢ƒæ—¶ï¼Œanaconda ä¼šåˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œå¹¶å°†è¯¥ç¯å¢ƒçš„æ‰€æœ‰ç›¸å…³è½¯ä»¶å­˜å‚¨åœ¨è¯¥ç›®å½•ä¸­ã€‚æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ›å»ºç¯å¢ƒã€‚

1.  æ‰“å¼€ Anaconda æç¤ºç¬¦(é»˜è®¤ç¯å¢ƒæ˜¾ç¤ºä¸º(base))å¹¶é”®å…¥ä»¥ä¸‹å†…å®¹:

```
conda create -n helloworld python==3.6.9conda activate helloworld
```

*ä½ å¯ä»¥åœ¨è¿™é‡Œ* *æ‰¾åˆ°ä¸åŒ conda å‘½ä»¤* [*çš„ cheetsheetã€‚*](https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf)

2.ä¸€æ—¦åˆ›å»ºå¹¶æ¿€æ´»äº†ç¯å¢ƒï¼Œæ‚¨éœ€è¦å®‰è£…ä»¥ä¸‹è½¯ä»¶ï¼Œè¿™äº›è½¯ä»¶æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡å®æ–½ ANN æ‰€éœ€è¦çš„ã€‚

```
pip install jupyterpip install pandas
pip install matplotlib
pip install seaborn
pip install tensorflow
```

ä¸Šè¿°è½¯ä»¶çš„å¿«é€Ÿä»‹ç»ã€‚

***Jupyter Notebook****å…è®¸ä½ ç¼–å†™ç°åœºä»£ç ï¼Œå¹¶ä¸ä»–äººåˆ†äº«ç¬”è®°æœ¬ã€‚ä»Šåä½ ä¼šç»å¸¸ç”¨åˆ°å®ƒã€‚*

****ç†ŠçŒ«*** æ˜¯ä¸€ä¸ª python æ•°æ®åˆ†æåº“ã€‚å…³é”®æ•°æ®ç»“æ„æ˜¯æ•°æ®å¸§ã€‚*

****Matplotlib*** å’Œ ***Seaborn*** æ˜¯æ•°æ®å¯è§†åŒ–åº“ï¼Œå¯¹äºæ¢ç´¢æ€§æ•°æ®åˆ†æéå¸¸æœ‰æ•ˆã€‚*

****Tensorflow*** æ˜¯ Google å¼€å‘çš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åº“ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå°†å®‰è£… tensorflow çš„ç¬¬ 2 ç‰ˆã€‚è¯·æ³¨æ„ï¼Œtensorflow çš„ b/w ç‰ˆæœ¬ 1 å’Œç‰ˆæœ¬ 2 ç•¥æœ‰ä¸åŒã€‚æˆ‘å»ºè®®ä½ ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ã€‚*

****Keras*** æä¾›é«˜çº§ APIï¼Œå¹¶ä¸ tensorflow 2 é›†æˆã€‚å®ƒæ˜¯ä¸€ä¸ªæ„å»ºåœ¨ tensorflow ä¹‹ä¸Šçš„ API åŒ…è£…å™¨ã€‚*

*3.ä¸€æ—¦æ‰€æœ‰çš„è½¯ä»¶å®‰è£…å®Œæ¯•ï¼Œåœ¨ anaconda æç¤ºç¬¦ä¸‹è¾“å…¥`jupyter notebook`ã€‚è¿™å°†æ‰“å¼€ä¸€ä¸ªæµè§ˆå™¨çª—å£ã€‚å³ä¸Šæ–¹ä¼šæœ‰ä¸€ä¸ª*ã€æ–°å»ºã€‘*ä¸‹æ‹‰ã€‚ä»ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©*â€œPython 3â€*ã€‚ä»æ–‡ä»¶èœå•ä¸­é€‰æ‹©*â€œå¦å­˜ä¸ºâ€*ï¼Œå°†ç¬”è®°æœ¬å¦å­˜ä¸ºâ€œhello worldâ€ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­å¥æ£€æŸ¥ tensorflow å’Œ Keras çš„ç‰ˆæœ¬ã€‚*

```
*import tensorflow as tf
print(f"tensorflow version: {tf.__version__}")
print(f"Keras version: {tf.keras.__version__}")*
```

*å¦‚æœä½ å·²ç»èµ°äº†è¿™ä¹ˆè¿œï¼Œç®¡é“å·¥ç¨‹å°±å®Œæˆäº†ğŸ˜Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡å®ç°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªäººå·¥ç¥ç»ç½‘ç»œã€‚ä½†é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ MNIST æ•°æ®é›†ã€‚*

# ***MNIST æ•°æ®é›†***

*MNIST æ•°æ®é›†åŒ…å« 60ï¼Œ000 å¹…è®­ç»ƒå›¾åƒå’Œ 10ï¼Œ000 å¹…æµ‹è¯•å›¾åƒã€‚æ•°æ®é›†åŒ…å«ä» 0 åˆ° 9 çš„æ‰‹å†™æ•°å­—ã€‚æ¯ä¸ªæ•°æ®ç‚¹æ˜¯ 28Ã—28 å¤§å°çš„ 2D é˜µåˆ—ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥é¢„æµ‹æ‰‹å†™æ•°å­—ã€‚è¿™æ˜¯æ•°æ®é›†çš„è§†è§‰å›¾åƒ(æ¥æº:[ç»´åŸºç™¾ç§‘](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png))*

*![](img/e7620493a3de60425cfd89c7d99be1f0.png)*

*Figure 1 â€” MNIST Dataset*

# ***æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå®‰å®ç°***

*ç°åœ¨ï¼Œæˆ‘ä»¬å‡†å¤‡å‡ºå‘äº†ğŸ˜ƒæˆ‘åªæ˜¯æƒ³è®©ä½ çŸ¥é“ï¼Œç°åœ¨å¯èƒ½è¿˜æœ‰ä¸€äº›äº‹æƒ…æ²¡æœ‰è¢«å¾ˆå¥½åœ°ç†è§£ã€‚æˆ‘å°†è¯•ç€ç®€è¦åœ°å†™ä¸€ä¸‹å®ƒä»¬ã€‚åœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­å°†ä¼šæ›´è¯¦ç»†åœ°ä»‹ç»å®ƒä»¬ã€‚è¿™é‡Œçš„ç›®çš„æ˜¯å‘æ‚¨ä»‹ç»æ·±åº¦å­¦ä¹ å·¥å…·é›†çš„ä¸–ç•Œä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚*

***åŠ è½½åº“å¹¶å¯¼å…¥ MINST æ•°æ®é›†:**ç¬¬ä¸€æ­¥æ˜¯åŠ è½½åº“å¹¶å¯¼å…¥æ•°æ®é›†ã€‚MNIST æ•°æ®é›†ç°åœ¨ä½œä¸º Keras æ•°æ®é›†çš„ä¸€éƒ¨åˆ†æ†ç»‘åœ¨ä¸€èµ·ã€‚ä¸‹é¢æ˜¯æ‰§è¡Œæ­¤æ“ä½œçš„ä»£ç :*

```
*import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns# Step 1\. Load train and test data set.
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data() # Step 2\. check the size of training and test datasets print(X_train_full.dtype, "-", X_train_full.shape)
print(y_train_full.dtype, "-", y_train_full.shape)
print(X_test.dtype, "-", X_test.shape)
print(y_test.dtype, "-", y_test.shape)# Step 3\. Randomly check one of the data points.
X_train_full[30]
y_train_full[30]*
```

*ä»£ç æ˜¯è‡ªæˆ‘è§£é‡Šçš„ï¼Œæ²¡æœ‰ä»€ä¹ˆå¤æ‚çš„ã€‚å½“æ‚¨æ‰§è¡Œ`X_train_full[30],`æ—¶ï¼Œå®ƒå°†æ˜¾ç¤ºä¸€ä¸ª 28 X 28 çš„ 2D æ•°ç»„ï¼Œæ•°å€¼èŒƒå›´ä» b/w 0 åˆ° 255ï¼Œå› ä¸ºæ¯ä¸ªæ•°æ®ç‚¹çš„å¤§å°éƒ½æ˜¯ 28 X 28ã€‚`dtype`æ˜¯`unit8`ï¼Œå®ƒä¿å­˜å€¼ b/w 0 å’Œ 255ã€‚*

***ç¼©æ”¾æ•°æ®å¹¶åˆ›å»ºéªŒè¯é›†:**ä¸‹ä¸€æ­¥æ˜¯ç¼©æ”¾æ•°æ® b/w 0 å’Œ 1 å¹¶åˆ›å»ºéªŒè¯æ•°æ®é›†ã€‚å¯¹äºéªŒè¯æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†æŠŠ`X_train_full, y_train_full`åˆ†ä¸º`X_valid, X_train`å’Œ`y_valid, y_train`ä¸¤ç»„ã€‚*

```
*# Scale the data b/w 0 and 1 by dividing it by 255 as its unsigned int
X_train_full = X_train_full/255.
X_test = X_test/255.# View the matrix now. The values will be b/w 0 and 1 X_train_full[30]# Create the validation data from training data.
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]X_train.shape
# should give o/p of (55000, 28, 28)X_valid.shape
# should give o/p of (5000, 28, 28)*
```

*æˆ‘ä»¬æœ‰ 5000 æ¡è®°å½•çš„éªŒè¯é›†å’Œ 55000 æ¡è®°å½•çš„è®­ç»ƒé›†ã€‚å¯¹äº†ï¼Œæˆ‘ä»¬åˆ°ç°åœ¨éƒ½æ²¡ç”¨è¿‡ matplotlib å’Œ seabornã€‚è®©æˆ‘ä»¬ç”¨å®ƒä»¬æ¥çœ‹çœ‹æœ±åº‡ç‰¹ç¬”è®°æœ¬ä¸­çš„å›¾åƒã€‚æ‰§è¡Œä¸‹é¢çš„ä»£ç æ¥æŸ¥çœ‹å®é™…å›¾åƒçš„è¾“å‡ºä»¥åŠå›¾åƒçš„çƒ­å›¾ã€‚*

```
*# view the actual image at index 30
plt.imshow(X_train[30], cmap='binary')*
```

*ä¸Šé¢çš„è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤º:*

*![](img/f87bb2f41ca4fb43891679552c5ccd15.png)*

*Figure 2: visual image at index 30*

```
*# Lets look at the pixels in detail using SNSplt.figure(figsize=(15,15))
sns.heatmap(X_train[30], annot=True, cmap='binary')*
```

*è¿™æ®µä»£ç æ˜¾ç¤ºäº†å®Œæ•´çš„ 28 X 28 ç½‘æ ¼ï¼Œæ¯ä¸ªåƒç´ çš„æ•°æ®å¦‚ä¸‹ã€‚*

*![](img/d5f56a458d91325fa25d3ec0b226a911.png)*

*Figure 3: Pixel view of the image*

***æ¨¡å‹æ„å»º:**ç°åœ¨æ˜¯æ—¶å€™æ„å»ºæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚æˆ‘ç¬¬ä¸€ç¯‡æ–‡ç« ä¸­çš„æ¦‚å¿µå°†æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å®ƒã€‚ä¸‹é¢æ˜¯æ„å»ºæ¨¡å‹çš„ä»£ç ã€‚*

```
*# lets create the model
# Flatten = make the array to sequential layer
# Dense = creating a hidden OR output layerLAYERS = [tf.keras.layers.Flatten(input_shape=[28,28],
name="inputLayer"),
         tf.keras.layers.Dense(300, activation="relu", name="hiddenLayer1"),
         tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer2"),
         tf.keras.layers.Dense(10, activation="softmax", name="outputLayer")]model = tf.keras.models.Sequential(LAYERS)*
```

*è¿™é‡Œå‘ç”Ÿäº†å¾ˆå¤šäº‹æƒ…ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹*

*è¾“å…¥å±‚:æˆ‘ä»¬å·²ç»å±•å¹³äº† 28 X 28 çš„è¾“å…¥çŸ©é˜µã€‚è¿™æ„å‘³ç€æ¯å¹…å›¾åƒå°†æœ‰ 28 x 28 = 784 ä¸ªè¾“å…¥å€¼ã€‚*

*éšè—å±‚:æˆ‘ä»¬ä½¿ç”¨â€œå¯†é›†â€æ¥åˆ›å»ºéšè—å’Œè¾“å‡ºå±‚ã€‚åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªéšè—å±‚ã€‚æˆ‘ä»¬åœ¨éšè—å±‚ 1 å’Œ 2 ä¸­æœ‰ 300 å’Œ 100 ä¸ªç¥ç»å…ƒã€‚è¿™äº›åªæ˜¯æˆ‘é€‰çš„éšæœºå€¼ã€‚ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ä»»ä½•å…¶ä»–å€¼ã€‚åœ¨åé¢çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Keras tuner è·å¾—è¿™äº›å€¼ã€‚æˆ‘åœ¨éšè—å±‚ä¸­ä½¿ç”¨â€œreluâ€æ¿€æ´»åŠŸèƒ½ã€‚å†è¯´ä¸€æ¬¡ï¼Œä»ç°åœ¨å¼€å§‹å°±è¿™æ ·åšã€‚éšç€æˆ‘ä»¬å¯¹æ¿€æ´»å‡½æ•°çš„äº†è§£ï¼Œæˆ‘ä»¬ä¼šå‘ç°æ›´å¤šçš„æ¿€æ´»å‡½æ•°ã€‚*

*è¾“å‡ºå±‚:è¾“å‡ºå±‚æœ‰ 10 ä¸ªç¥ç»å…ƒï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ä¸­æœ‰å€¼ b/w 0 å’Œ 9ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤„ç†å¤šç±»åˆ†ç±»é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¾“å‡ºå±‚ä½¿ç”¨â€œsoftmaxâ€ä½œä¸ºæ¿€æ´»ã€‚*

*ä½ å¯ä»¥æƒ³è±¡æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œå¦‚ä¸‹:*

*![](img/da872e87ef9ee16b96d8e6e6d7bfa9d7.png)*

*Figure 4: Deep neural network with 2 hidden layers*

*ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹çš„æ¦‚è¦ã€‚å®ƒæœ‰å¾ˆå¤šä¿¡æ¯éœ€è¦æ¶ˆåŒ–ã€‚æ‰§è¡Œ jupyter ç¬”è®°æœ¬ä¸­çš„ä»£ç `model.summary()`ã€‚æ‚¨åº”è¯¥ä¼šçœ‹åˆ°ä¸‹é¢çš„è¾“å‡ºã€‚*

*![](img/ba942ca6ce69c869136c858ec72ce06d.png)*

*Figure 5: Model Summary*

*å®ƒæ˜¾ç¤ºäº† 3 åˆ—ã€‚â€œå±‚â€åˆ—æ˜¾ç¤ºäº†å±‚çš„åç§°ã€‚â€œè¾“å‡ºå½¢çŠ¶â€æ æ˜¾ç¤ºæ¯å±‚ä¸­çš„ç¥ç»å…ƒæ•°é‡ã€‚â€œParam #â€æ˜¯éœ€è¦ç†è§£çš„åˆ—ã€‚é‡Œé¢æœ‰ä¸€äº›éšæœºæ•°ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹ã€‚*

*Param #æ˜¯æƒé‡å’Œåå·®çš„è®¡ç®—ã€‚åœ¨ hiddenLayer1 ä¸­ï¼Œæˆ‘ä»¬æœ‰ 300 ä¸ªç¥ç»å…ƒä» inputLayer çš„ 784 ä¸ªç¥ç»å…ƒæ¥æ”¶è¾“å…¥ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬æœ‰ 300 X 784 = 235200 ä¸ªé‡é‡ã€‚åå·®ç­‰äºè¯¥å±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡ã€‚åœ¨ hiddenLayer1ï¼Œæ˜¯ 300ã€‚æ‰€ä»¥ï¼Œå¦‚æœä½ æŠŠæƒé‡å’Œåå·®åŠ èµ·æ¥ï¼Œä½ ä¼šå¾—åˆ° 235500 (235200 + 300)ã€‚ç±»ä¼¼åœ°ï¼Œå…¶ä»–å±‚çš„å€¼å¯ä»¥è®¡ç®—å¦‚ä¸‹:*

```
*# Param # (Nodes in layer A * Nodes in layer B + Bias)
# hiddenLayer1 = 784*300 + 300 = 235500
# hiddenLayer2 = 300*100 + 100 = 30100
# outputLayer = 100*10 + 10 = 1010# Trainable Params = 235500 + 30100 + 1010 = 266610*
```

*å¯è®­ç»ƒå‚æ•°æ˜¯å¯ä»¥è¢«ä¿®æ”¹ä»¥è®­ç»ƒæ¨¡å‹çš„æƒé‡å’Œåå·®çš„æ€»æ•°ã€‚å¦‚æœä½ æŠŠä»¥ä¸Šæ•°å­—åŠ èµ·æ¥ï¼Œä½ ä¼šå¾—åˆ° 266ï¼Œ610ã€‚æˆ‘å¸Œæœ›è¿™ä¸€ç‚¹ç°åœ¨å·²ç»éå¸¸æ¸…æ¥šäº†ã€‚è®©æˆ‘ä»¬ç»§ç»­ã€‚*

***æƒé‡å’Œåå·®:**è®©æˆ‘ä»¬æ¥çœ‹çœ‹æƒé‡å’Œåå·®ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥æŸ¥çœ‹åˆå§‹åˆ†é…çš„æƒé‡å’Œåå·®ã€‚*

```
*hidden1 = model.layers[1]
weights, biases = hidden1.get_weights()#weights should be a metrics of 784 X 300 and biases should be 300
weights.shape
biases.shapeprint(weights)
print(biases)*
```

*å½“æ‚¨æ‰§è¡Œ print è¯­å¥æ—¶ï¼Œæ‚¨å°†çœ‹åˆ°æƒé‡çš„éšæœºå€¼å’Œæ‰€æœ‰åå·®çš„ 0 å€¼ã€‚å½“æ¨¡å‹åœ¨åå‘ä¼ æ’­æœŸé—´å¼€å§‹å­¦ä¹ æ—¶ï¼Œè¿™äº›ä¿¡æ¯è¢«æ›´æ–°ã€‚*

***æŸå¤±å‡½æ•°ï¼Œåå‘ä¼ æ’­ä¼˜åŒ–å™¨:**æˆ‘ä»¬ç°åœ¨éœ€è¦å®šä¹‰åå‘ä¼ æ’­çš„æ“ä½œã€‚æˆ‘ä»¬éœ€è¦è®¾ç½®è¦ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€æ›´æ–°æƒé‡å’Œåå·®çš„ä¼˜åŒ–å™¨ä»¥åŠå‡†ç¡®æ€§çš„åº¦é‡æ ‡å‡†ã€‚*

```
*LOSS_FUNCTION = "sparse_categorical_crossentropy"
OPTIMIZER = "SGD"
METRICS = ["accuracy"]model.compile(loss=LOSS_FUNCTION,
             optimizer=OPTIMIZER,
             metrics=METRICS)*
```

*æˆ‘ä½¿ç”¨`sparse_catagorical_crossentropy`ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä½¿ç”¨`stochastic gradient descent`ä½œä¸ºä¼˜åŒ–å™¨ã€‚æˆ‘å°†åœ¨ä»¥åçš„æ–‡ç« ä¸­å†™æ›´å¤šå…³äºè¿™äº›çš„å†…å®¹ã€‚æŒ‡æ ‡æŒ‡å®šäº†æˆ‘ä»¬æƒ³è¦ç”¨æ¥ä½œä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„åº¦é‡çš„å‚æ•°ã€‚ä¸€æ—¦æ‰€æœ‰è¿™äº›éƒ½é€‰æ‹©å¥½äº†ï¼Œæˆ‘ä»¬å°±åœ¨æ¨¡å‹ä¸Šè°ƒç”¨ç¼–è¯‘æ–¹æ³•ã€‚*

***æ¨¡ç‰¹è®­ç»ƒ:**æ˜¯æ—¶å€™è®­ç»ƒæˆ‘ä»¬çš„æ¨¡ç‰¹äº†ï¼Œçœ‹çœ‹å¥¹çš„è¡¨ç°å¦‚ä½•ã€‚æˆ‘ä»¬éœ€è¦ç†è§£ä¸€ä¸ªæ–°æœ¯è¯­ï¼Œå®ƒå«åšçºªå…ƒã€‚ç®€å•åœ°è¯´ï¼Œepoch æ˜¯æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´å¿…é¡»è¢«è¯„ä¼°çš„æ¬¡æ•°ã€‚*

```
*EPOCHS = 30
VALIDATION_SET = (X_valid, y_valid)history = model.fit(X_train, y_train, epochs=EPOCHS,
                   validation_data=VALIDATION_SET)*
```

*åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº† 30 ä¸ªå†å…ƒï¼Œè¿™æ„å‘³ç€æ¨¡å‹å¿…é¡»è¿›è¡Œ 30 æ¬¡æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚éªŒè¯é›†ç”¨äºå¯¹ç…§è®­ç»ƒæ•°æ®é›†éªŒè¯æ¨¡å‹ã€‚å½“æ‰§è¡Œä»£ç æ—¶ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä¸‹è¾“å‡º:*

```
*Epoch 1/30
1719/1719 [==============================] - 5s 3ms/step - loss: 0.6110 - accuracy: 0.8479 - val_loss: 0.3095 - val_accuracy: 0.9162
Epoch 2/30
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2867 - accuracy: 0.9175 - val_loss: 0.2354 - val_accuracy: 0.9360
Epoch 3/30
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2328 - accuracy: 0.9341 - val_loss: 0.2017 - val_accuracy: 0.9450
Epoch 4/30
1719/1719 [==============================] - 6s 3ms/step - loss: 0.1986 - accuracy: 0.9433 - val_loss: 0.1725 - val_accuracy: 0.9506
...
...
...
...
Epoch 30/30
1719/1719 [==============================] - 6s 3ms/step - loss: 0.0285 - accuracy: 0.9937 - val_loss: 0.0681 - val_accuracy: 0.9808*
```

***æ‰¹æ¬¡å¤§å°å’Œæ‰¹æ¬¡æ•°é‡:**å½“æ¨¡å‹è¢«è®­ç»ƒæ—¶ï¼Œå®ƒä¸ä¼šåœ¨æ¯æ¬¡è¿­ä»£ä¸­ä¼ é€’ä¸€ä¸ªè¾“å…¥ã€‚ç›¸åï¼Œå®ƒéœ€è¦æ‰¹é‡å¤§å°ã€‚`fit`æ–¹æ³•æœ‰ä¸€ä¸ª batch_size å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œè¿™ä¸ªå‚æ•°é»˜è®¤ä¸º 32ã€‚å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè€ƒè™‘åˆ°æ‰¹é‡å¤§å°ä¸º 32ï¼Œè®­ç»ƒé›†ä¸º 55000ï¼Œæˆ‘ä»¬å¾—åˆ°çš„æ‰¹é‡æ•°ä¸º 1719ã€‚*

*ä»¥ä¸‹æ˜¯è¾“å‡ºçš„æ‰€æœ‰å‚æ•°çš„ç®€è¦è¯´æ˜:*

```
*# Epoch 1/30
# 1719/1719 [==============================] - 5s 3ms/step - loss: 0.6110 - accuracy: 0.8479 - val_loss: 0.3095 - val_accuracy: 0.9162# default batch size=32
# No. of batches = X_train.shape/batch_size = 55000/32 = 1719# 1719 = No of batches
# 5s = 5 seconds for one single Epoch
# 3ms/step = time taken for one batch
# loss: 0.6110 = training loss (summation of all losses in all batches)
# accuracy: 0.8479 = training accuracy (summation for all batches)
# val_loss: 0.3095 = validation loss
# val_accuracy: 0.9162 = validation accuracy*
```

*å½“æ‚¨è§‚å¯Ÿæ¨¡å‹è®­ç»ƒçš„è¾“å‡ºæ—¶ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°å‡†ç¡®æ€§åœ¨æ¯ä¸ªå†å…ƒä¹‹åéƒ½åœ¨æé«˜ã€‚è¿™è¡¨æ˜è¯¥æ¨¡å‹é€šè¿‡è°ƒæ•´ä½œä¸ºå¯è®­ç»ƒå‚æ•°çš„æƒé‡å’Œåå·®æ¥å­¦ä¹ ã€‚æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°çœ‹åˆ°æ¨¡å‹å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´æ•è·çš„`history`æ¥å‡å°‘æŸå¤±å’Œæé«˜å‡†ç¡®æ€§ã€‚ä¸‹é¢æ˜¯ä»£ç å’Œå¯è§†åŒ–è¡¨ç¤º*

```
*pd.DataFrame(history.history).plot(figsize=(8,5))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()*
```

*![](img/5cd0f0a85ff8f47a735e45d5422a6ffd.png)*

*Figure 5: loss and accuracy*

*ä»ä¸Šå›¾å¯ä»¥æ¸…æ¥šåœ°çœ‹å‡ºï¼Œåœ¨ 20 ä¸ªæ—¶æœŸ(x è½´)ä¹‹åï¼Œæ¨¡å‹æ²¡æœ‰å­¦åˆ°å¤šå°‘ä¸œè¥¿ã€‚æˆ‘ä»¬æœ‰ä¼˜åŒ–çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­è®¨è®ºå¦‚ä½•ä¼˜åŒ–ã€‚*

*æ¨¡å‹æµ‹è¯•:ç°åœ¨è®©æˆ‘ä»¬æ ¹æ®æˆ‘ä»¬åœ¨å¼€å§‹æ—¶åˆ›å»ºçš„æµ‹è¯•æ•°æ®æ¥æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚å®ç°è¿™ä¸€ç‚¹çš„ä»£ç éå¸¸ç®€å•ã€‚*

```
*# validate against test data now
model.evaluate(X_test, y_test)#Output:
#313/313 [==============================] - 1s 2ms/step - loss: #0.0734 - accuracy: 0.9763*
```

*ä»è¾“å‡ºä¸­å¯ä»¥çœ‹å‡ºï¼ŒæŸå¤±å’Œç²¾åº¦éå¸¸æ¥è¿‘éªŒè¯æ•°æ®é›†(val _ loss:0.0681-val _ accuracy:0.9808)ã€‚æˆ‘ä»¬å¯ä»¥å¯¹æ­¤è¿›è¡Œè°ƒæ•´ï¼Œä½†è¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚*

*ç°åœ¨è®©æˆ‘ä»¬ä»æµ‹è¯•æ•°æ®é›†ä¸­æŠ½å–ä¸€äº›æ ·æœ¬ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¾—åˆ°äº†æ­£ç¡®çš„é¢„æµ‹:*

```
*X_new = X_test[:3]
y_pred = np.argmax(model.predict(X_new), axis=-1)
y_test_new = y_test[:3]for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('off')
    plt.show()
    print("---"*20)*
```

*æˆ‘ä»¬ä»æµ‹è¯•æ•°æ®ä¸­æå–å‰ 3 ä¸ªå€¼ï¼Œå¹¶å°è¯•é¢„æµ‹è¿™äº›å€¼ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é¢„æµ‹å€¼ä¸å¾ªç¯ä¸­çš„å®é™…å€¼è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘çš„ 3 ä¸ªé¢„æµ‹éƒ½æ˜¯æ­£ç¡®çš„ã€‚ä½ å¯ä»¥å°è¯•ä¸€äº›å…¶ä»–çš„éšæœºæ ·æœ¬ï¼Œæˆ–è€…å°†ä¸€äº›æ‰‹å†™çš„å›¾åƒè½¬æ¢æˆ 28 X 28 åƒç´ ï¼Œçœ‹çœ‹è¿™ä¸ªæ¨¡å‹çš„è¡¨ç°å¦‚ä½•ã€‚*

*è¿™å°±å®Œæˆäº†æˆ‘ä»¬æ·±åº¦å­¦ä¹ çš„*â€˜hello worldâ€™*ã€‚åœ¨ç»“æŸè¿™ç¯‡æ–‡ç« ä¹‹å‰ï¼Œæˆ‘æƒ³å±•ç¤ºå¦ä¸€ä¸ªå·¥å…·ï¼Œå®ƒå¯ä»¥éå¸¸æ–¹ä¾¿åœ°å¯è§†åŒ–æ£€æŸ¥æ¨¡å‹ã€‚è¿™ä¸ªå·¥å…·å«åš***â€˜Netronâ€™ã€‚ä½ æ—¢å¯ä»¥ä» [github](https://github.com/lutzroeder/netron) ä½ç½®ä¸‹è½½ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ç½‘å€[https://netron.app/](https://netron.app/)ç›´æ¥åœ¨æµè§ˆå™¨çª—å£æ‰“å¼€ä½ ä¿å­˜çš„æ¨¡å‹ã€‚æˆ‘å°†è®©æ‚¨æ›´å¤šåœ°æ¢ç´¢è¿™ä¸ªå·¥å…·ï¼Œå®ƒå¯¹äºåˆæ¬¡å­¦ä¹ æ¥è¯´éå¸¸æ–¹ä¾¿ã€‚****

*ä½ å¯ä»¥å‚è€ƒ jupyter ç¬”è®°æœ¬çš„ MNIST å®ç°[è¿™é‡Œ](https://github.com/srinivaskulkarni2020/deep-learning/blob/main/MyFirstANN.ipynb)ã€‚*

*å—¯â€¦â€¦å¦‚æœä½ è¯»åˆ°è¿™ä¸€è¡Œï¼Œå¹¶ä¸”è‡³å°‘ç†è§£äº†æ–‡ç« çš„ 40%,ä½ å°±å·²ç»è¾¾åˆ°äº†å‰è¿›çš„ç›®æ ‡ã€‚ä¸æ–­å­¦ä¹ ã€‚*