# ä½¿ç”¨ selenium çš„åŠ¨æ€ç½‘é¡µæŠ“å–:æŠ“å–å—ä¿æŠ¤çš„ç½‘ç«™

> åŸæ–‡ï¼š<https://medium.com/geekculture/dynamic-web-scraping-using-selenium-scrape-protected-websites-401de3497939?source=collection_archive---------2----------------------->

![](img/bb202319cab1345a965173a5854b9c43.png)

Designed by upklyak

## ä½†æ˜¯ï¼æˆ‘ä»¬ä¸Šæ¬¡å·²ç»è®¨è®ºè¿‡äº†ï¼ğŸ˜¤

åœ¨æˆ‘çš„ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ beautiful soup ä»ç½‘é¡µä¸­æŠ“å–ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯èƒ½å·²ç»è¦†ç›–äº†å‡ ä¹æ¯ä¸ªé‡è¦çš„æ–¹é¢å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬åŒ…æ‹¬æ‰€æœ‰ä¸åŒçš„ç±»å‹ï¼Œçˆ¬è¡Œï¼Œåƒµå°¸ä¿æŠ¤ï¼Œè‡ªåŠ¨åŒ–ï¼Œé€šè¿‡ NLP æŠ“å–ï¼Œä½†æ˜¯ï¼ç¾æ±¤åªèƒ½å¤„ç†é™æ€ç½‘é¡µï¼Œä¸èƒ½å¤„ç†åŠ¨æ€*éº¦å…‹é£æ‰è½*ğŸ¤

> å¦‚æœä½ æ˜¯åˆ®æ“¦çš„æ–°æ‰‹ï¼Œæˆ–è€…éœ€è¦äº†è§£ä¸€äº›é«˜çº§æ¦‚å¿µï¼Œå¯ä»¥çœ‹çœ‹è¿™ç¯‡æ–‡ç« 

[https://medium . com/geek culture/get-started-with-web-scraping-for-data-analysis-Fe 55 cc 0 b 8d 61](/geekculture/getting-started-with-web-scraping-for-data-analysis-fe55cc0b8d61)

## ğŸ¥²ï¼Œé™æ€å’ŒåŠ¨æ€ç½‘é¡µæœ‰ä»€ä¹ˆåŒºåˆ«

ä½ é—®äº†ä¸€ä¸ªå¾ˆæ£’çš„é—®é¢˜ï¼é™æ€ç½‘é¡µä½¿ç”¨ç›´æ¥åµŒå…¥ä½ ä»£ç ä¸­çš„å†…å®¹ï¼Œè€Œåœ¨åŠ¨æ€ç½‘é¡µä¸­ï¼Œ**ä» javascript ç”Ÿæˆ**æ•°æ®ï¼Œå¹¶å°†å®ƒä»¬æ”¾å…¥ DOM å…ƒç´ ä¸­ã€‚ä¿æŒç®€å•

## å¥½çš„åœºæ™¯æ—¶é—´ğŸ«£

è€ƒè™‘ä½ æ­£åœ¨æŠ“å–ä½¿ç”¨åˆ†é¡µå…ƒç´ çš„ç½‘ç«™ï¼Œä½†æ˜¯ä½ çŒœå¯¹äº†ï¼Œå½“æˆ‘ä»¬ç‚¹å‡»é“¾æ¥æ—¶ï¼Œurl æ²¡æœ‰æ›´æ–°ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨çˆ¬è¡Œï¼ğŸ˜£ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬è¯•ç€ç”¨ç¾ä¸½çš„æ±¤åšæ¯ä¸€ä»¶å¯èƒ½çš„äº‹æƒ…ï¼Œä½†å®ƒä¸èµ·ä½œç”¨ï¼ğŸ˜–è¿™æ„å‘³ç€æ‚¨æ²¡æœ‰æ‰¾åˆ°æ ¹æ®è¯·æ±‚åŠ è½½æ•°æ®çš„ HTML ç‰‡æ®µï¼ğŸ˜«ä¹Ÿå°±æ˜¯è¯´æ²¡æœ‰ Nutella äº†ğŸ˜©â€¦ç­‰ç­‰ï¼Œçº³ç‰¹æ‹‰æ˜¯çˆ±

## ä¸ºä»€ä¹ˆæ˜¯ç¡’å‘¢ğŸ¤”

å¾ˆé«˜å…´ä½ é—®äº†ğŸ¤—ä¸ä»…ä»…æ˜¯ç¡’ï¼Œè¿˜æœ‰ç¡’+ç¾ä¸½çš„æ±¤*æ—§çˆ±ä»ç„¶æœ‰ä¸€äº›è¸¢å®ƒ*ç¡’åœ¨ç®€å•çš„æœ¯è¯­ä¸­æ¨¡æ‹Ÿç”¨æˆ·ä¸æµè§ˆå™¨çš„äº¤äº’ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œselenium æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒå¯ä»¥è¿è¡Œå’Œæ‰§è¡Œè„šæœ¬ï¼Œå¹¶é€šè¿‡å‘è¿æ¥æ‚¨çš„æµè§ˆå™¨å’Œ Selenium çš„ web é©±åŠ¨ç¨‹åºå‘é€å’Œæ¥æ”¶æ–¹æ³•è°ƒç”¨å’Œæ•°æ®æ¥æ§åˆ¶æ‚¨çš„ Web æµè§ˆå™¨ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œæˆ‘å¿…é¡»å®‰è£… Chromeï¼Œä½†æ˜¯æ˜¯çš„ï¼Œå¦‚æœä½ æ²¡æœ‰ Chrome æˆ–è€… Firefoxï¼Œç°åœ¨å°±å®‰è£…å®ƒä»¬ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä½ çŸ¥é“çš„å„ç§è‡ªåŠ¨åŒ–åº”ç”¨ç¨‹åºéƒ½ä½¿ç”¨ seleniumã€‚

## å¼€å§‹å‰çš„æµ‹éªŒæ—¶é—´ğŸ˜Œ

å¦‚æœä½ å»è¿‡åœ°çƒï¼Œä½ å¯èƒ½çŸ¥é“æ¾³å¤§åˆ©äºšï¼Œé‚£é‡Œæœ‰ä¸€ä¸ªæ‹¥æœ‰ä»“åº“çš„åŒ–å­¦å®¶ã€‚æ­å–œä½ çŸ¥é“æˆ‘ä»¬æ­£åœ¨åˆ®çš„ç½‘ç«™ï¼ğŸ¥³

## å®‰è£…å…ˆå†³æ¡ä»¶ğŸ› 

```
# Run the commands below once 
!pip install beautifulsoup4
!pip install requests
!pip install selenium
!pip install webdriver-manager
```

## è·å¾—æˆ‘ä»¬çš„è¿›å£ğŸ˜Œ

```
import requests
from bs4 import BeautifulSoup
import pandas
import time
from selenium import webdriver
#dynamic scraping using selenium and beautiful soup
options = webdriver.ChromeOptions()
options.add_argument('--ignore-certificate-errors')
options.add_argument('--incognito')
options.add_argument('--headless')
driver = webdriver.Chrome("*insert_your_directory*/chromedriver", chrome_options=options)
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
driver = webdriver.Chrome(ChromeDriverManager().install())
```

è¯·ç¡®ä¿æ‚¨æ‹¥æœ‰æ­£ç¡®çš„ webdriver è·¯å¾„ã€‚æˆ‘æ›´å–œæ¬¢ä»ç½‘ä¸Šæ‰‹åŠ¨ä¸‹è½½ç½‘ç»œé©±åŠ¨ç¨‹åºå¹¶ä¿å­˜åœ¨ pwd ä¸­ã€‚

## å¬å”¤ 3 ç¡’è®©æˆ‘åšåˆ°äº†è¿™ä¸€ç‚¹ğŸ‘»

æˆ‘ä»¬å°†è®©æµè§ˆå™¨ç‚¹å‡»ä¸‹ä¸€æ­¥æŒ‰é’®ï¼Œé€šè¿‡ selenium ä» **Selenium è·å–æºä»£ç ï¼Œè€Œä¸æ˜¯åƒä¸Šæ¬¡é‚£æ ·è¯·æ±‚**ã€‚è¿™åº”è¯¥å¯è¡Œï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨æ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œå°±åƒå¦‚æœæ¯é¡µæœ‰ 63 é¡µï¼Œæˆ‘ä»¬å°†å•å‡»â€œä¸‹ä¸€æ­¥â€æŒ‰é’®â€œæŠ“å–â€!*java è„šæœ¬åŠ è½½å†…å®¹*åˆ®ï¼java è„šæœ¬åŠ è½½å†…å®¹å†æ¬¡åˆ®ï¼é‡å¤å¥½ç¡’å°†å¼€å§‹ä½¿ç”¨æ‚¨çš„æµè§ˆå™¨ï¼Œç°åœ¨å®ƒçš„æ‹¥æœ‰

```
driver.get("The_link_that_you_found_via_quiz")
src = driver.page_source
soup = BeautifulSoup(src, 'lxml')
final_list = []
df = []
for i in range(62):
    src = driver.page_source
    soup = BeautifulSoup(src, 'lxml')
    product = soup.find_all('div', class_='product')
    l = []
    for item in product:
        d={}
        d["Name"] = (item.find('div', class_ ='product__title').text.replace("[","").replace("]",""))
        d["Price"] = (item.find('span', class_ ='product__price-current').text.replace("[","").replace("]",""))
        d["Discount"] = (item.find('em', class_ ='product__price-discount'))
        l.append(d)

    final_list.extend(l)

    next_button = driver.find_elements(by=By.CLASS_NAME, value="pager__button--next")try:
        if next_button[0].is_displayed():
            driver.execute_script("arguments[0].click();",next_button[0])
            time.sleep(1)
    except:
        pass

df = pandas.DataFrame(final_list)df.to_csv("xoxo.csv")
df.to_excel("xoxo.xlsx")
```

## å†è§ğŸ‘‹

æ€»ç»“ä¸€ä¸‹ï¼Œæ²¡æœ‰å¤ªå¤šè¦è§£é‡Šçš„äº†ï¼Œä½†è¿™åªæ˜¯æ·±å…¥äº†è§£ selenium çš„å¼€å§‹ï¼Œå› ä¸º selenium ä¸ä»…ä»…é™äº web æŠ“å–ï¼Œè¿˜è®°å¾—æ¯æ¬¡ GPU åœ¨ 1 ç§’å†…å”®ç½„å—ï¼Ÿç”šè‡³æ˜¯ NFT æ°ç—‡ï¼Ÿç¡’ä½ å¼„è„äº†æˆ‘çš„æœ‹å‹ã€‚ä½†æ˜¯æ¯å¤©éƒ½æ˜¯å­¦ä¹ æ–°ä¸œè¥¿å’Œå¿«ä¹ç¼–ç ğŸ«¡çš„æœºä¼š